\graphicspath{{./images/}}      
\def\CHAPTERONE{./chapters/Chapter-1} 

\chapter{Zielsetzung und Motivation}
\label{chap:motivation}
%	\input{\CHAPTERONE /motivation}

% Sinn und Zweck der Arbeit
% Was ist das Ziel? Was will ich herausfinden?
% Warum ist diese Arbeit sinnvoll?
% Was kann mit den Ergebnissen erreicht werden?
\glsresetall

Von Menschen bewohnte Umgebungen, wie eine Küche, sind starken nichtdeterministischen Veränderungen unterworfen. Den einen Morgen werden Cornflakes gegessen, den nächsten ein Knuspermüsli. Eine Packung Eistee wird nach dem Benutzen nicht wieder zurück an den ursprünglichen Ort gestellt. Ein autonomer Roboter, der in einer solchen Haushaltsumgebung operieren soll, muss mit der Fähigkeit ausgestattet sein, die Szenerie, die sich ihm offenbart, zu interpretieren. Genauer gesagt, muss er Objekte in einer solchen Szene identifizieren können, um die ihm gestellte Aufgabe zu bewältigen. Soll er die Packung Müsli wieder wegräumen, muss er erst einmal herausfinden, wo sie sich jetzt auf dem Tisch befindet. Dann muss er auch noch erkennen, um welche Packung es sich handelt, um sie wieder an ihren ursprünglichen Platz zu stellen. Dabei greift der Roboter auf eine Form einer \gls{kb} zurück, um die Objekte auf Grundlage seiner Wahrnehmungssysteme zu identifizieren. Die beiden  Packungen Müsli sind beide box-artig, jedoch unterscheidet sich die Packung Cornflakes in ihrer gelben Farbe von dem grünen Knuspermüsli. Mit dem Wissen aus der \gls{kb}, kann der Roboter nun schlussfolgern, dass die derzeit auf dem Tisch stehende Packung die Cornflakes sind. Das Trainieren solcher \glspl{kb} ist jedoch aufwendig, denn um die Eigenschaften und Unterschiede der einzelnen Objekte effektiv herausarbeiten zu können, muss eine große Menge an Trainingsdaten zur Verfügung stehen. Solche Trainingsdaten könnten zum Beispiel Bilder einer Küche mit verschiedene Müsli Packungen an verschiedenen Orten gepaart mit verschiedenen Schüsseln und Milchsorten sein. Eine große Anzahl solcher Bilder ist nun vonnöten, da Eigenschaften eines Objektes unterschiedlich wahrgenommen werden können, bedingt durch so etwas wie unterschiedliche Hintergründe, Lichteinfall, Position und Rotation des Objektes oder der Kamera und weitere Störfaktoren. \newline
Das Erstellen künstlicher Trainingsdaten könnte diesen Prozess vereinfachen, indem eine große Menge unterschiedlicher Bilder schnell erstellt werden könnte. Für optimale Ergebnisse sollten die künstlichen Trainingsdaten eine möglichst große Ähnlichkeit mit der echten Umgebung aufweisen, damit es nicht zu Verwirrung und Fehlern bei der Klassifikation der Objekte kommt. Eine künstliche Packung Cornflakes sollte also die gleichen Eingenschaften, zum Beispiel eine gelbe Textur und eine box-artige Form, aufweisen wie die echte Packung. \glspl{gameengine} sind mittlerweile in der Lage, fotorealistische virtuelle Welten zu schaffen und könnten damit ein gutes Mittel sein, künstliche Trainingsdaten zu erstellen.  

\section{Perzeption}
In der Robotik beschreibt die Perzeption die Wahrnehmung des Roboters. Damit sich ein Roboter in einer Umgebung überhaupt zurechtfinden kann, ist sie unerlässlich. Häufig werden \gls{rgb}-Kamerabilder als Eingabe benutzt, um darauf Perzeptionsalgorithmen anzuwenden, die versuchen Objekte in den wahrgenommen Szenen zu finden und zu identifizieren. Im Folgenden werden einige Systeme und Ansätze dazu vorgestellt.

\subsection{Systeme zur Objekterkennung}

In \cite{multimodalTemplate} werden Objekte durch \textit{Template-Matching} identifiziert. Template-Matching beschreibt dabei den Abgleich des wahrgenommen Objektes mit gespeicherten Vorlagen und darin gespeicherten \glspl{feature}n. Im Gegensatz zum simplen Template-Matching werden im LINE-MOD Algorithmus aus \cite{multimodalTemplate} mehrere Modalitäten/Informationen aus verschiedenen Quellen in den Vorlagen gespeichert. Aus Farbbildern kann der Farbgradient und aus Tiefenbildern die Normalen der Oberfläche als \gls{feature} gewonnen werden. Im Gegensatz zu traditionellen Lernverfahren für \glspl{kb} ist das Extrahieren der \gls{feature} und das Anlegen einer Vorlage schnell gemacht. Durch die \gls{feature} aus verschiedenen Quellen wird auch eine hohe Robustheit und Erkennungsrate erreicht, da sie sich gegenseitig ergänzen und so Hintergrundstörfaktoren weniger ins Gewicht fallen. \par

Das MOPED-\gls{framework} \cite{moped} versucht Objekterkennung in Szenen mit hoher Komplexität zu realisieren. Dabei wird auf eine Datenbank zurückgegriffen, die 3D-Modelle von Objekten enthält. Diese wurden aus Bildern mit verschiedenen Ansichten der echten Objekte erstellt. Der Vorgang ist allerdings nicht vollautomatisch, sondern braucht menschliche Aufsicht. Wichtiger Bestandteil des MOPED-\glspl{framework} ist das \textit{Iterative Clustering-Estimation (ICE)}. Damit wird versucht, extrahierte \glspl{feature} des Bildes einem Objekt in der Datenbank zuzuordnen und die Pose des Objektes ermittelt. Dazu werden \gls{feature}, die wahrscheinlich zu demselben Objekt gehören, zu Gruppen zusammengefasst, und innerhalb der Gruppen nach Objekthypothesen gesucht. Iterativ werden dann Gruppen, die basierend auf ihrer Pose zum gleichen Objekt gehören, wieder vereinigt. Ein Durchlauf von MOPED wendet dabei mehrere Iterationen von ICE an, was es erlaubt, falsche Hypothesen zu erkennen und doch noch richtig zuzuordnen. Die Iterationen können auch einfach parallelisiert werden, wodurch MOPED eine geringere Latenz bei der Online-Erkennung als andere \glspl{framework} aufweist.  \par

Eine bessere Erkennungsrate als MOPED bietet das Verfahren in \cite{3DCNNObjRec}. Hier werden \gls{cnn}-Modelle mit 3D-Modellen von Objekten trainiert, die dann zur Objekterkennung in Bildern benutzt werden. Die Erkennung basiert wie MOPED auch auf \glspl{feature} der Objekte. Die 3D-Modelle werden mit \glspl{feature} angereichert und ein Trainingsdatenset wird erstellt, indem die Modelle vor verschiedenen Hintergründen in verschiedenen Posen abgebildet werden. Bei der Erkennung werden \gls{feature} aus den 2D-Bildern extrahiert und den 3D-Modellen zugeordnet. Damit gute Erkennungsraten erhalten werden, müssen jedoch akkurate 3D-Modelle zur Verfügung stehen und für jede Objektinstanz ein Modell, denn sonst können einzelne Objekte nicht auseinandergehalten werden.   \par

Das 3DNet \gls{framework} \cite{3dnet} bietet Objektklassifikation und Posenerkennung basierend auf einer Datenbank mit 3D-\gls{cad}-Modellen. Dazu werden Deskriptoren mit den \gls{cad}-Modellen trainiert. In Echtzeit können nun Objekte in Bildern der Microsoft Kinect Kamera klassifiziert werden. Das \gls{framework} basiert dabei auf der \gls{pcl}\cite{pcl} und lässt sich in \acrshort{ros} integrieren. Außerdem werden dabei einige Vorteile, die das Training mit 3D-Modellen bietet, aufgezeigt. Die Modelle sind vollständig, da sie aus jedem Blickwinkel angesehen werden können, und parametrisierbar, da neue Trainingsdaten mit anderen Blickwinkeln oder Störgeräuschen einfach erstellt werden können. Gleichzeitig sind die Modelle auch sensorenunabhängig, denn es können die Wahrnehmungseigenschaften eines jeden Sensors simuliert werden. Außerdem wird der Zugriff auf zusätzliche Informationen erlaubt, die nicht in echten Bildern oder Objekten vorhanden sind, wie die Entropie der Blickwinkel.  \par 

Auch in \citep{modelsWWW} werden 3D-\gls{cad}-Modelle verwendet, um einen Roboter Büromöbel identifizieren zu lassen. Anders als bei den vorherigen Methoden werden allerdings keine kleinen \gls{feature} betrachtet, sondern die Klassifikation auf Basis von Objektteilen. Dies sind bei einem Stuhl zum Beispiel Lehne, Beine und Sitzfläche. Dies bietet Vorteile gerade bei starker Okklusion und wenn verschiedene Blickwinkel vorliegen. Die Modelle werden aus dem World Wide Web bezogen, da Hersteller und Hobbykünstler gute Modelle zur Verfügung stellen. Für ein Möbelstück wird nur ein Modell benötigt, da damit verschiedene Ansichten vom Roboter erstellt werden können, mit denen er dann trainiert wird. Repräsentiert werden die Modelle intern von einem Vokabular der einzelnen Teilstücke sowie einer räumlichen Anordnung innerhalb spezifischer Modelle. Die Klassifikation findet dann über einen Abgleich der wahrgenommen Teile mit einem Voting, um was für ein Teil es sich handeln könnte, statt. Mit den identifizierten Teilen wird dann geschaut, zu welchem \gls{cad}-Modell die Anordnung der Teile am ehesten passen könnte. So ist der Roboter auch in der Lage, unbekannte Möbel als Stuhl oder Tisch zu identifizieren, wenn die charakteristische Anordnung von Teilstücken übereinstimmt.   

\subsection{Attribut-basierte Objekterkennung}
\label{sec:aboi}

Die Verwendung von \glspl{feature} in Bildern zur Identifikation von Objekten hat einen entschiedenen Nachteil: Merkmale von unbekannten Objekten können zwar wahrgenommen werden, auf Grund eines fehlenden Modells oder einer fehlenden Referenz jedoch mit keinem Objekt in der \gls{kb} verknüpft werden. Somit kann das unbekannte Objekt auch nicht erkannt werden. \newline
Die Attribut-basierte Objekterkennung basiert auf der Art und Weise, wie der Mensch Objekte beschreibt, erkennt und unterscheidet. Eine Tasse hat die Form eines Zylinders, ist in der Regel im Vergleich zu anderen Objekten eher klein und hat einen Henkel. Wird einem Menschen ein Objekt so beschrieben, kann er daraus ableiten, dass es sich um eine Tasse handelt. Auch bis dato unbekannte Instanzen von Tassen können sofort als solche identifiziert werden. In der Perzeption können Attribute wie Form, Größe, Farbe oder das Vorhanden sein von bestimmten Charakteristika ausgenutzt werden, um Objekte zu identifizieren, zu beschreiben und zu katogorisieren. Sieht der Roboter ein Objekt mit den eben erwähnten Attributen, handelt es sich wahrscheinlich um eine Tasse. Durch das Vorhandensein eines Henkels kann der Roboter wiederum ableiten, was er mit der Tasse machen könnte, weil er schon weiß, wie man mit anderen Objekten mit Henkel umgeht. Auch über unbekannte Objekte, deren Namen nicht bekannt sind, können damit Aussagen getätigt werden, und aus Beschreibungen in natürlicher Sprache können neue Objekte und Kategorien erlernt werden. \cite{descObjbyAtr, atrBasedObjIden} \par

Um die einzelnen Attribute und ihre visuellen Aspekte zu beschreiben, wird trotzdem auf \gls{feature} zurückgegriffen. In \cite{descObjbyAtr} werden die \gls{feature} Farbe und Textur, Visual Words und Kanten als Grundlagen für die Attribute Form, Teile und Material benutzt. Das Attribut Form gibt nun Aussagen zu Objekten wie: \glqq ist eine Box\grqq \xspace oder \glqq ist ein Zylinder\grqq. Teile beschreibt kleinere zusammengehörende Teile eines Objekts wie: \glqq hat einen Kopf\grqq \xspace oder \glqq hat ein Rad\grqq. Material beschreibt, woraus Objekte bestehen: \glqq besitzt Holz\grqq \xspace oder \glqq ist pelzig\grqq. Um die einzelnen Objekte noch zusätzlich unterscheiden zu können, da die gewählten Attribute nicht immer eine eindeutige Unterscheidung zulassen (\textit{Hund oder Katze?}), werden zusätzlich noch Unterscheidungsattribute verwendet. Diese Attribute wurden verwendet, um Objekte zu kategorisieren, bekannte sowie auch unbekannte Objekte zu beschrieben und neue Kategorien zu erlernen. Wichtig dabei ist ein gute Generalisierung der Attribute über Kategorien hinweg. Wird ein \gls{klassifikator} für Räder mit Bildern von Autos und Bussen trainiert, kann es sein, dass er nicht lernt, Räder zu finden, sondern metallische Flächen, da bei Autos und Bussen die Räder häufig von Metall umgeben sind. Bei einer hölzernen Kutsche ist es ihm nun nicht möglich, das Rad zu erkennen. Um dies zu verhindern und damit eine höhere Generalität über Kategorien hinweg zu erreichen, werden \glspl{klassifikator} nur mit ausgewählten Merkmalen trainiert. Dazu wird ein \gls{klassifikator} nicht nur mit Bildern von Rädern trainiert, sondern auch mit Bildern, die keine Räder enthalten, um so das Lernen des korrelierenden Attributs (\textit{metallisch}) zu vermeiden.  \par    

In \cite{atrBasedObjIden} wird Attribut-basierte Objekterkennung benutzt, um Objekte in einer Szene basierend auf einem Satz zu identifizieren. Die 110 Objekte werden dazu in 12 Kategorien eingeteilt. Die benutzten Attribute sind: Farbe, Form, Material und Name. Die entsprechenden Bezeichnungen für die Objekte wurden von Arbeitern von \texttt{Amazon Mechanical Turk,} einem Service, um rund um die Uhr die Intelligenz von menschlichen Arbeitern zu nutzen, gegeben. Dabei stellte sich heraus, dass gerade die Objektnamen sehr unterschiedlich ausfallen, da jeder ein Objekt anders benennt und damit auch eine große Menge an potenziellen Namen zusammen kommt. Die Experimente zeigen eine hohe Erkennungsrate und Robustheit von Attribut-basierter Objekterkennung, sowie auch, dass eine Kombination verschiedener Attributklassifikatoren bessere Ergebnisse liefert als die jeweils Einzelnen. Um so mehr Objekte in den Datensätzen vorkommen, um so geringer wird auch die Erkennungsrate - allerdings fällt die Rate bei dem Ensemble der \glspl{klassifikator} weniger stark ab als bei den Einzelnen. Es wird wie in \cite{descObjbyAtr} auch versucht, das Lernen des korrelierenden Attributs zu vermeiden, und die daraus resultierenden \glspl{klassifikator} werden verwendet, um neue Attributwerte eines Attributs zu lernen.

\section{Virtuelle Realität}

\begin{quote}
\glqq I define a virtual reality experience as any in which the user is effectively immersed in a responsive virtual world. This implies user dynamic control of viewpoint.\grqq \newline
 \hfill - Frederick P. Brooks
\end{quote}
So beschreibt Frederick P. Brooks 1999 \cite{brooks} den Begriff der \glspl{vr}, der auch heute noch zutreffend ist. In den letzten Jahren ist mit dem Zugriff auf günstige Rechenleistung und massentaugliche \textit{Head-mounted Displays} wie der \textsc{HTC Vive}\footnote{\url{https://www.vive.com/de/}} oder der \textsc{PlayStation VR}\footnote{\url{https://www.playstation.com/de-de/explore/playstation-vr/}} die Möglichkeit einer effektiven Immersion und dynamischen Kontrolle innerhalb der virtuellen Welt gestiegen. Aus dem Blickwinkel der Robotik betrachtet, ist es für den Roboter allerdings schwierig, so etwas wie Immersion zu spüren. Stattdessen sollte \gls{vr} als Möglichkeit einer vollkommen beherrschbaren Simulation angesehen werden, die im Sinne der Immersion fotorealistische Bilder bietet.  \par

4 Jahre vor Brooks Definition von \gls{vr} werden in \cite{burger1995} die Probleme bei der Entwicklung von robust und zuverlässig arbeitenden autonomen Robotern aufgezeigt: sie müssen ausgiebig getestet und verifiziert werden. Das sei aber häufig schwer zu bewerkstelligen, da der Roboter während der Entwicklung selber meistens nicht zur Verfügung stehe, Training und Tests nur mit einer begrenzten Anzahl und vorher aufgenommenen Bildern stattfinde und für lernende Systeme das Training zeitintensiv sei und realistische Testumgebungen mit \gls{gt} für die Daten nicht zur Verfügung stünden. Um den Problemen beizukommen, sprechen die Autoren sich für das Benutzen von Simulationen aus. Diese würden Folgendes bieten: 
\begin{itemize}
	\item Ausführliches Trainieren und Testen in unterschiedlichen, realistischen und kompletten Umgebungen.
	\item Die \gls{gt} Informationen stehen zur Verfügung.
	\item Autonomes Lernen ist ohne menschliche Intervention möglich.
	\item Große Mengen von Daten können einfacher erstellt werden.
	\item \textit{Exploratory Learning}, wobei Trainingsdaten während des Lernprozesses als Reaktion generiert werden können. 
	\item geringe Kosten
\end{itemize}

Daneben stehen aber auch einige Nachteile, die in \cite{heisele} aufgeführt werden. So fehle es an akkuraten und realistischen 3D-Modellen, was sich aber durch günstige 3D-Scanner und den Zugriff auf Modelle von Hobbykünstlern und gewerbliche Modelle aus Computerspielen oder Filmen lösen ließe. Auch seien hochauflösende synthetische Daten nicht immer realistisch genug oder zu aufwendig zu generieren.  \par

Dass Bilder aus einer Virtuellen Welt sich genauso gut zum Trainieren und Testen von Perzeptionssystemen eignen können, zeigt \cite{kaneva}. Um die Deskriptoren, die bei der Erkennung \gls{feature} aus den Bildern extrahieren, effektiv vergleichen zu können, werden Bilder einer Szene aus verschiedenen Blickwinkeln und mit unterschiedlichen Lichtverhältnissen benötigt. Da es schwierig ist, solche Bilder aufzunehmen, wird eine fotorealistische virtuelle Welt benutzt. Der Vergleich der Performance der jeweiligen Deskriptoren auf echten und unechten Bildern zeigt, dass es nahezu keinen Unterschied gibt. \par 

Auch in \cite{synthImg} wird gezeigt, dass sich synthetisch erzeugte Bilder zum Training eines \gls{klassifikator}s eignen. Allerdings wird dabei keine Virtuelle Realität genutzt und es geht auch nicht darum, möglichst echte beziehungsweise fotorealistische Bilder zu erzeugen. Stattdessen werden aus einer kleinen Menge echter Bilder, die Drohnen zeigen, mit absichtlich nicht detailgetreuen 3D-Modellen dieser Drohnen möglichst ähnliche synthetische Bilder erzeugt. Dazu werden die Modelle in den Hintergrund eingefügt und auf den Bildern automatisch Postprocessing, wie MotionBlur und Noise, angewandt. Nun wird die Ähnlichkeit zum echten Bild mit einer Funktion gemessen, die Bilder nach ihrer Eignung für ein effektives Training einstuft. Sollte das synthetische Bild eine passende Ähnlichkeit aufweisen, werden aus dem Bild weitere erzeugt, indem die Position und Orientierung des Drohnen-Modells verändert werden. Die eigentliche Klassifikation basiert wieder auf \glspl{feature} der Objekte. Die durchgeführten Experimente zeigen, das so trainierte \glspl{klassifikator} bessere Ergebnisse liefern als mit echten Bildern trainierte \glspl{klassifikator}. Vor allem reichen bereits 12 reale Bilder aus, um genügend synthetische Bilder zu erzeugen, sodass der mit ihnen trainierte \gls{klassifikator}, der bessere Ergebnisse liefert, als ein mit $\geqq 12 * 8$ realen Bildern trainierter \gls{klassifikator}. \par   

Eine andere Möglichkeit, \gls{vr} für die Robotik zu nutzen, zeigt \cite{imitationLearning2}. Um dem Roboter Aktionen beizubringen, kann \textit{Imitation Learning} verwendet werden. Der Mensch ist ein Experte für alltägliche oder einfache Arbeiten. Beim Ausführen solcher Aufgaben können Daten, wie er Aufgaben bewältigt, gesammelt, analysiert und Modelle daraus erstellt werden. Mit dieses Modellen versucht dann der Roboter, die Aufgaben zu bewältigen. Da es jedoch für den Mensch langweilig ist, immer die gleiche Aufgabe auszuführen, um genügend Lerndaten zu erhalten, wurde ein Spiel entwickelt, dass die Aufgabe spielerisch widerspiegelt. So wird versucht, die Motivation hochzuhalten und den Spieler zu ermutigen, die Aufgaben möglichst genau und realistisch auszuführen. Das Spiel läuft in einer virtuellen Umgebung ab, wodurch auch auch physikalische Effekte observiert und interpretiert werden können. Gleichzeitig kann der Roboter neue Situation/Aufgaben erstellen und dem Spiel hinzufügen. \newline
\cite{imitationLearning1} benutzt den gleichen Ansatz, führt den Spielcharakter der Simulation jedoch noch weiter aus, indem ein freihändig steuerbares 3D-Tower-Defense Game in einer \gls{gameengine} programmiert wurde. Damit können die Daten aus einer kontrollierten und sicheren Umgebung gewonnen werden. Durch die Verwendung von spieletypischen Konzepten wie das Bewältigen immer schwierigerer Aufgaben durch Taktik und allmähliche Verbesserung seiner Ressourcen, hat der Spieler noch mehr Anreize weiterzuspielen und damit akkurate und hochqualitative Daten zu liefern. 


\section{Inferenz}

Das Ausführen von komplexen alltäglichen Aufgaben erfordert ein breites Wissen über die beteiligten Objekte, die Umgebung und den eigentlichen Ablauf der Aufgabe. In einer dynamischen von Menschen bewohnten Haushaltsumgebung sind diese Aspekte jedoch nicht immer oder nicht eindeutig gegeben. Die Aufgabe \glqq Räume den Küchentisch ab\grqq \xspace ist für einen Menschen zwar eindeutig, für einen Roboter jedoch nicht spezifisch genug. Es muss also eine Möglichkeit geben, die ihm fehlenden Informationen aus seiner Wahrnehmung und seinem  Wissen in der \gls{kb} zu schlussfolgern. Mit einem Blick auf den Küchentisch könnte er so inferieren, welche Objekte wegzuräumen sind. Daraus kann er wiederum schließen, wo er hinfahren muss, um sie wegzustellen, und auch wie er diese Objekte anzufassen hat. Jetzt erst kann er sich einen Plan erstellen, um die Aufgabe zu bewältigen, bevor er diesen dann ausführt. Es ist also essentiell für den Roboter, auf generelles Wissen über seine Umgebung, aber auch auf Präferenzen und vergangene Erfahrungen zugreifen zu können und über sie zu schlussfolgern, um so zu einer Entscheidung zu kommen. Dazu wird dem Roboter ein \textit{Reasoning-Mechanismus} mitgegeben. Dieser arbeitet auf Modellen, die als \gls{kb} fungieren und das Wissen repräsentieren. \cite{Tenorth2010}    \par

Damit ein Roboter weiß, wie er ein Objekt anzufassen oder damit umzugehen hat, muss er die \textit{funktionalen Teile} der Objekte, wie einen Griff, finden. In \cite{reasoningFuncParts} wird über \gls{cad}-Modelle der Objekte geschlussfolgert, um diese funktionalen Teile aufzufinden. Die Modelle werden in Teile segmentiert, welche in einer \gls{kb} hinterlegt werden. Aus den Teilen in der \gls{kb} werden die übergeordneten Konzepte der funktionalen Teile abgeleitet, namentlich Behälter, Griffe und Ablagen. Nun können Objekte mit einer passenden Perzeptionskomponente wahrgenommen und identifiziert werden und die funktionalen Teile der Objekte inferiert werden. So kann zum Beispiel bestimmt werden, wo ein Löffel angefasst werden sollte, oder Behälter mit einem bestimmten Füllvermögen gefunden werden. \par

In \cite{pronobis1} sind verschiedene Informationen in einer \gls{kb} hinterlegt, und ein Roboter soll sich mit dem Wissen in einer Büroumgebung zurechtfinden, also aus seiner Wahrnehmung mit Hilfe seiner Reasoning-Komponente die Räume identifizieren. Das Wissen setzt sich zusammen aus Objektattributen, das Aussehen von Räumen, eine topologische Struktur sowie menschliche Eingaben. Die \gls{kb} besteht aus 4 Schichten: 
\begin{enumerate}
	\item \textit{kategorische Schicht:} enthält Modelle von Objekten, Räumen und Orientierungspunkten.
	\item \textit{konzeptuelle Schicht:} Commonsense Wissen und eine Taxonomie von räumlichen Konzepten. Hier ist Wissen abgespeichert, wie: Milch ist in der Küche. Die Darstellung erfolgt in relationalen Beziehungen: \textit{Küche hat-objekt Milch}.
	\item \textit{Eigenschaften des Raums:} ein Ort bekommt Attribute, um ihn zu beschreiben.
	\item \textit{konzeptuelle Map} Dies ist die \gls{kb} in Form eines graphischen Modells und erlaubt damit das Schlussfolgern über unbekannte Räume. 
\end{enumerate}
Der Roboter wurde mit Modellen aus ähnlichen Räumen trainiert und dann mit unbekannten Räumen getestet. Während des Tests sammelte er Informationen über Form, Größe und Aussehen der Räume sowie Objekte innerhalb seiner Umgebung und versuchte mit diesen Informationen oder dem Nichtvorhandensein von erwarteten Informationen zu schlussfolgern. Dabei konnten viele Räume korrekt identifiziert werden, was für das Zusammenspiel verschiedener Informationsquellen beim Schlussfolgern über Unbekanntes spricht. \par

Eine Erweiterung des \robosherlock-\glspl{framework} (siehe Kapitel \ref{sec:robosherlock} auf S.\pageref{sec:robosherlock}) zur Erkennnung von Objekten und das Schlussfolgern über die wahrgenommene Szene wird in \cite{pr2looking} vorgestellt. Dabei wird Attribut-basierte Objekterkennung benutzt, um die wahrgenommen Eigenschaften der Objekte zu beschreiben. In einem ersten Schritt werden Objekthypothesen in der Szene aufgestellt, zu denen spezialisierte Experten Informationen extrahieren. Diese umfassen die Attribute Farbe, Größe, Form, Text und Logo. Mit diesem Wissen wird eine gemeinsame Wahrscheinlichkeitsverteilung in Form eines \gls{mln} trainiert. Aus einer wahrgenommenen Szene kann nun Wissen geschlussfolgert werden, indem die Szene als Evidenz zu einer Anfrage verwendet wird. In den Experimenten wurden 21 Objektkategorien in 50 Szenen mit jeweils 5-10 Objekten pro Szene erstellt und damit das Vorgehen getestet. Zusätzlich wurden die Szenen in 4 Küchenszenarien (Frühstück, Mittagessen, Blick in den Kühlschrank und einen Schrank) eingeteilt. Jede Szene wurde manuell mit dem Wissen, in welchem Szenario man sich befindet, angereichert, um so Aufgaben-spezifisches Wissen zu modellieren. Die Grundlegende Anfrage bezog sich auf die Kategorie der Objekte, es wurde also aus der wahrgenommen Szene mit dem \gls{mln} inferiert, um welche Objekte es sich wohl handelt. Das Verfahren erreicht eine hohe Klassifikationsrate, was vor allem auf das Zusammenspiel der einzelnen Experten als Ensemble, sie ergänzen sich gegenseitig, und die Eigenschaften von \glspl{mln}, über alle Objekte in der Szene gleichzeitig schlussfolgern und das gemeinsame Auftreten von Objekten innerhalb einer Szene modellieren zu können, zurückgeführt wird.

\section{Ziel der Arbeit}
\label{sec:goal}

Um Robotern zu erlauben, sich in dynamischen Umgebungen zurechtzufinden und so alltägliche Aufgaben ausführen zu können, müssen sie mit passenden Wahrnehmungssystemen ausgestattet sein. Um Objekte mit verschiedenen Eigenschaften und Charakteristika eindeutig identifizieren zu können, ist das Schlussfolgern über die wahrgenommene Szene und eine \gls{kb} von Vorteil. Das Trainieren ist jedoch zeitintensiv, da eine große Menge an Trainingsdaten erstellt werden muss \cite{burger1995}. \glspl{gameengine} bieten mittlerweile die Möglichkeit, fotorealistische Bilder in einer \acrlongpl{vr} zu rendern, mit denen eine \gls{kb} anstatt echter Bilder trainiert werden kann. Gleichzeitig können durch die volle Kontrolle über die virtuelle Umgebung verschiedene Szenarien aus verschiedenen Blickwinkeln unter verschiedenen Bedingungen aufgenommen und so schnell eine große Menge an Trainingsdaten erstellt werden. \par 

Ziel dieser Arbeit ist es, herauszufinden, ob sich fotorealistische Renderings zum Trainieren einer \gls{kb} eignen, die zur Objekterkennung eingesetzt wird. Dazu werden in der \unreal Szenen erstellt und gerendert, die ein Roboter in einer Küche wahrnehmen könnte. Aus diesen Szenen werden Bilder aus verschiedenen Blickwinkeln aufgenommen. Diese werden von dem Perzeptionsframework \robosherlock interpretiert, indem die Objekte segmentiert und ihre Eigenschaften und damit das Wissen über sie als Attribute extrahiert werden. Diese bilden die logischen Prädikate, um die \gls{kb} in Form eines \gls{mln} zu trainieren. \glspl{mln} bieten Eigenschaften, die ihnen erlauben, effektiv für die Objektklassifikation eingesetzt zu werden. Mit dem resultierenden \gls{mln} wird über wahrgenommene Szenen geschlussfolgert und so die Performance bei der Objektklassifikation mit fotorealistische synthetischen Bildern evaluiert.       
