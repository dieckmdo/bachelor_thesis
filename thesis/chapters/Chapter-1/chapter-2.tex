\graphicspath{{./images/}}      
\def\CHAPTERONE{./chapters/Chapter-1} 

\chapter{Zielsetzung und Motivation}
\label{chap:motivation}
%	\input{\CHAPTERONE /motivation}

% Sinn und Zweck der Arbeit
% Was ist das Ziel? Was will ich herausfinden?
% Warum ist diese Arbeit sinnvoll?
% Was kann mit den Ergebnissen erreicht werden?

Von Menschen bewohnte Umgebungen, wie eine Küche, sind starken nichtdeterministischen Veränderungen unterworfen. Den einen morgen werden Cornflakes gegessen, den nächsten ein Knuspermüsli. Eine Packung Eistee wird nach dem benutzen nicht wieder zurück an den selben Ort gestellt. Ein autonomer Roboter der in einer solchen Haushaltsumgebung operieren soll, muss mit der Fähigkeit ausgestattet sein, die Szenerie, die sich ihm offenbart zu interpretieren. Genauer gesagt, muss er Objekte in einer solchen Szene identifizieren können, um die ihm gestellte Aufgabe zu bewältigen. Soll er die Müsli Packung wieder wegräumen, muss er erst einmal herausfinden, wo sie sich jetzt auf dem Tisch befindet, und auch erkennen, um welche Packung es sich handelt, um sie wieder an ihren richtigen Platz zu stellen.  Dabei greift der Roboter auf eine Form einer \gls{kb} zurück, um die Objekte auf Grundlage seiner Wahrnehmungssysteme zu identifizieren. Die beiden Müsli Packungen sind beide Box-artig, jedoch unterscheidet sich die Cornflakes Packung in ihrer gelben Farbe von der grünen Knuspermüsli Packung. Mit dem Wissen aus der \gls{kb}, kann der Roboter nun schlussfolgern, dass die noch dort stehende Packung eine Cornflakes Packung ist. ein Das Trainieren solcher \glspl{kb} ist jedoch aufwendig, da eine große Menge an Trainingsdaten für gute Resultate zur Verfügung stehen muss beziehungsweise erstellt werden müssen. Zum Beispiel Szenen in denen verschiedene Müsli Packungen an verschiedenen Orten gepaart mit verschiedenen Schüsseln und Milchsorten stehen. So können die Unterschiede besser herausgearbeitet und die einzelnen Objekte und ihre Zusammenhänge besser erlernt werden. Das Erstellen künstlicher Trainingsdaten könnte diesen Prozess vereinfachen. Für optimale Ergebnisse sollten die künstlichen Trainingsdaten eine möglichst große Ähnlichkeit mit der echten Umgebung aufweisen, damit er nicht zu Verwirrung und Fehlern bei der Klassifizierung kommt. Eine künstliche Cornflakes Packung sollte also die gleichen Eingenschaften, also zum Beispiel eine gelbe Textur und eine box-artige Form, aufweisen, wie die echte Packung. Virtuelle Realität ist in der Lage fotorealistische Szenen zu schaffen und könnte damit ein gutes Mittel sein, künstliche Trainingsdaten zu erstellen.  

\section{Perzeption}
In der Robotik beschreibt die Perzeption die Wahrnehmung des Roboters. Damit sich ein Roboter in einer Umgebung überhaupt zurechtzufinden kann, ist sie unerlässlich. Am häufigsten werden \gls{rgb}-Kamerabilder als Eingabe benutzt, um darauf Perzeptionsalgorithmen laufen zu lassen, die Objekte in den wahrgenommen Szenen zu finden und zu erkennen versuchen. Im folgenden werden einige Systeme und Ansätze dazu vorgestellt.

\subsection{Systeme zur Objekterkennung}

In \cite{multimodalTemplate} werden Objekte durch \textit{Template-Matching} identifiziert. Template-Matching beschreibt dabei den Abgleich des Wahrgenommen Objektes mit gespeicherten Vorlagen und darin gespeicherten Hinweisen. Im Gegensatz zum simplen Template-Matching werden im LINE-MOD Algorithmus aus \cite{multimodalTemplate} mehrere Modalitäten/Informationen aus verschiedenen Quellen in den Vorlagen gespeichert. Aus Farbbildern kann der Farbgradient als Hinweis und aus dem Tiefenbild können die Normalen der Oberfläche gewonnen werden. Im Gegensatz zu traditionellen Lernverfahren für \glspl{kb} ist das extrahieren der Hinweise und das anlegen einer Vorlage schnell gemacht. Durch die Hinweise aus verschiedenen Quellen wird auch eine hohe Robustheit und Erkennungsrate erreicht, da sie sich gegenseitig ergänzen und so Hintergrundstördaten weniger ins Gewicht fallen. \par

Das MOPED-Framework \cite{moped} versucht Objekterkennung in Szenen mit hoher Komplexität zu realisieren. Dabei wird auf eine Datenbank zurückgegriffen, die 3D-Modelle von Objekten enthält. Diese wurden aus Bildern mit verschiedenen Ansichten der echten Objekte erstellt. Der Vorgang ist allerdings nicht vollautomatisch, sondern braucht menschliche Aufsicht. Wichtiger Bestandteil des MOPED-Frameworks ist das \textit{Iterative Clustering-Estimation (ICE)}. Damit wird versucht, extrahierte Merkmale des Bildes einem Objekt in der Datenbank zuzuordnen und die Pose des Objektes zu ermitteln. Dazu werden Merkmale, die wahrscheinlich zu einem Objekt gehören, zu Gruppen zusammengefasst, und darin nach Objekthypothesen gesucht. Iterativ wird dann geschaut, ob Gruppen basierend auf ihrer Pose zum gleichen Objekt gehören und dann vereinigt. Ein Durchlauf von MOPED wendet dabei mehrere Iterationen von ICE an, was es erlaubt falsche Hypothesen zu erkennen und sie den richtigen zuzuordnen. Die Iterationen können auch einfach parallelisiert werden, wodurch MOPED eine geringere Latenz bei der Online-Erkennung als andere Frameworks aufweist.  \par

Eine bessere Erkennungsrate als MOPED bietet das Verfahren in \cite{3DCNNObjRec}. Hier werden \gls{cnn}-Modelle mit 3D-Modellen von Objekten trainiert, die dann zur Objekterkennung in Bildern benutzt werden. Die Erkennung basiert wie MOPED auch auf Merkmalen. Die 3D-Modelle werden mit Merkmalen angereichert und um ein Trainingsdatenset zu erhalten, werden die sie vor verschiedenen Hintergründen in verschiedenen Posen abgebildet. Bei der Erkennung werden Merkmale aus den 2D-Bildern extrahiert und den 3D-Modellen zugeordnet werden. Damit gute Erkennungsraten erhalten werden, müssen jedoch akkurate 3D-Modelle zur Verfügung stehen und für jede Objektinstanz ein Modell, denn sonst können einzelne Objekte nicht auseinandergehalten werden.   \par

Das 3DNet Framework \cite{3dnet} bietet Objektklassifizierung und Posenerkennung basierend auf einer Datenbank mit 3D-\gls{cad}-Modellen. Dazu werden Deskriptoren mit den \gls{cad}-Modellen trainiert. In Echtzeit können nun Objekte in Bildern der Microsoft Kinect Kamera klassifiziert werden. Das Framework basiert dabei auf der \gls{pcl}\cite{pcl} und lässt sich in \acrshort{ros} integrieren. Außerdem werden einige Vorteile, die das Training mit 3D-Modellen bietet, aufgezeigt:  
\begin{itemize}
	\item Vollständigkeit
	\item Parametrisierbarkeit
	\item Sensoren Unabhängigkeit
	\item Zugriff auf zusätzliche Informationen
\end{itemize}

Im Gegensatz zu \cite{3DCNNObjRec} werden in \cite{synthImg} absichtlich nicht ganz detailgetreue 3D-Modelle benutzt, um mit ihnen synthetische Bilder zum Training eines Klassifizierers zu generieren. Als Grundlage dienen dazu eine kleine Menge echter Bilder von Drohnen. Aus diesen Bildern werden mit den 3D-Modellen der Drohnen möglichst ähnliche synthetische Bilder erzeugt. Dazu werden die Modelle in den Hintergrund eingefügt und auf den Bildern automatisch Postprocessing, wie MotionBlur und Noise, angewandt. Die Ähnlichkeit wird mit einer Funktion gemessen und sollte das synthetische Bilder nach der Funktion eine passende Ähnlichkeit aufweisen, werden aus dem Bild weitere erzeugt, indem die Position und Orientierung des Drohnen-Modells verändert werden. Die eigentliche Klassifizierung basiert wieder auf Merkmalen der Objekte, deshalb sollte das Ziel der Ähnlichkeitsfunktion auch nicht sein, möglichst schöne und echt Bilder zu generieren, sondern Bilder die effektiv für das Training sind. Experimente zeigen das so trainierte Klassifizierer bessere Ergebnisse liefern als mit echten Bildern trainierte Klassifizierer. Vor allem reichen bereits 12 reale Bilder, um genügend synthetische Bilder zu erzeugen, um einen Klassifizierer zu trainieren, der bessere Ergebnisse liefert, als ein mit $\geqq 12 * 8$ realen Bildern trainierter Klassifizierer. \par    

Auch in \citep{modelsWWW} werden 3D-\gls{cad}-Modelle verwendet, um einen Roboter Büromöbel identifizieren zu lassen. Anders als bei den vorherigen Methoden werden allerdings keine kleinen Merkmale betrachtet, sondern die Klassifizierung auf Basis von Objektteilen. Dies sind bei einem Stuhl zum Beispiel Lehne, Beine und Sitzfläche. Dies bietet Vorteile gerade bei starker Okklusion und wenn verschiedene Ansichten vorliegen. Die Modelle werden aus dem World Wide Web bezogen, da Hersteller und Hobbykünstler gute Modelle zur Verfügung stellen. Für ein Möbelstück wird nur ein Modell benötigt, da damit verschiedene Ansichten vom Roboter erstellt werden können, mit denen er dann trainiert wird. Repräsentiert werden die Modelle intern von einem Vokabular der einzelnen Teilstücke sowie einer räumlichen Anordnung innerhalb spezifischer Modelle. Die Klassifizierung findet dann über einen Abgleich der wahrgenommen Teile mit einem Voting, um was für ein Teil es sich handeln könnte, statt, bevor geschaut wird, zu welchem \gls{cad}-Modell die Anordnung der Teile am ehesten passen könnte. So ist der Roboter auch in der Lage unbekannte Möbel als Stuhl oder Tisch zu identifizieren, wenn die charakteristische Anordnungen von Teilstücken übereinstimmt.   

\subsection{Attribut-basierte Objekterkennung}
\label{sec:aboi}

Die Verwendung von Merkmalen in Bildern zur Identifikation von Objekten hat einen entschiedenen Nachteil: Merkmale von unbekannten Objekten können zwar wahrgenommen werden, die Merkmale können auf Grund eines fehlenden Modells oder Referenz in der \gls{kb} jedoch mit keinem Objekt verknüpft werden. Somit kann das unbekannte Objekt auch nicht erkannt werden. \newline
Die Attribut-basierte Objekterkennung basiert auf der Art und Weise, wie der Mensch Objekte beschreibt, erkennt und unterscheidet. Eine Tasse hat die Form eines Zylinders, ist in der Regel im Vergleich zu anderen Objekten eher klein und hat einen Henkel. Wird einem Menschen ein Objekt so beschrieben, kann er daraus ableiten, dass es sich um eine Tasse handelt. Auch bis dato unbekannte Instanzen von Tassen können sofort als solche wahrgenommen werden. In der Perzeption können Attribute, wie Form, Größe, Farbe oder das vorhanden sein von bestimmten Charakteristika, ausgenutzt werden, um Objekte zu identifizieren, zu beschreiben und zu katogorisieren. Sieht der Roboter ein Objekt mit den eben erwähnten Attributen, handelt es sich wahrscheinlich um eine Tasse. Durch das Vorhandensein eines Henkels kann der Roboter widerum ableiten, was er mit der Tasse machen könnte, weil er schon weiß wie man mit anderen Objekten mit Henkel umgeht. Auch über unbekannte Objekte, deren Namen nicht bekannt ist, können damit Aussagen getätigt werden und aus Beschreibungen in natürlicher Sprache können neue Objekte und Kategorien gelernt werden. \cite{descObjbyAtr, atrBasedObjIden} \par

Um die einzelnen Attribute und ihre visuellen Aspekte zu beschreiben, wird trotzdem auf Merkmale zurückgegriffen. In \cite{descObjbyAtr} werden die Merkmale Farbe und Textur, Visual Words und Kanten als Grundlagen für die Attribute Form, Teile und Material benutzt. Das Attribute Form gibt nun Aussagen zu Objekten, wie \glqq ist eine Box\grqq \xspace oder \glqq ist ein Zylinder\grqq. Teile beschreibt kleinere zusammengehörende Teile eines Objekts, wie \glqq hat einen Kopf\grqq \xspace oder \glqq hat ein Rad\grqq. Material beschreibt, woraus Objekte bestehen, wie \glqq besitzt Holz\grqq \xspace oder \glqq ist pelzig\grqq. Um die einzelnen Objekte noch zusätzlich unterscheiden zu können, da die gewählten Attribute nicht immer eine eindeutige Unterscheidung zulassen (\textit{Hund oder Katze?}) werden noch Unterscheidungsattribute dazu genommen. Mit diesen Attributen wurden in Experimenten nun Objekte kategorisiert, bekannte sowie auch unbekannte Objekte beschrieben und neue Kategorien erlernt. Wichtig dabei ist ein gute Generalisierung der Attribute über Kategorien hinweg. Wird ein Klassifizierer für Räder mit Bildern von Autos und Bussen trainiert, kann es sein, dass er nicht lernt Räder zu finden sondern metallische Flächen, da bei Autos und Bussen die Räder häufig von Metall umgeben sind. Soll er nun ein Rad bei einer hölzernen Kutsche finden, versagt er kläglich. Um dies zu verhindern und damit eine höhere Generalisierung über Kategorien hinweg zu erreichen, werden Klassifizierer nur mit ausgewählten Merkmalen trainiert, was auch gute Ergebnisse liefert. Dazu wird der Klassifizierer nicht nur mit Bildern mit Rädern trainiert, sondern auch mit Bildern, die keine Räder enthalten, umso zu vermeiden, das korrelierende Attribut zu lernen.   \par    

In \cite{atrBasedObjIden} wird Attribut-basierte Objekterkennung benutzt, um Objekte in einer Szene basierend auf einem Satz zu identifizieren. Die 110 Objekte werden dazu in 12 Kategorien eingeteilt. Die benutzen Attribute sind: Farbe, Form, Material und Name. Die entsprechenden Bezeichnungen für die Objekte wurden von Arbeitern von Amazon Mechanical Turk, einem Service, um rund um die Uhr die Intelligenz von menschlichen Arbeitern zu nutzen, gegeben. Dabei stellte sich heraus, dass gerade die Objektnamen sehr unterschiedlich ausfallen, da jeder ein Objekt anders benennt und damit auch eine große Menge an potenziellen Namen zusammenkommt.  Die Experimente zeigen eine hohe Erkennungsrate und Robustheit von Attribut-basierte Objekterkennung als auch, dass eine Kombination verschiedener Attributklassifizierer, bessere Ergebnisse liefert, als die jeweils Einzelnen. Umso mehr Objekte in den Datensätzen vorkommen, umso geringer wird auch die Erkennungsrate - allerdings fällt die Rate bei dem Ensemble der Klassifizierer weniger stark ab, als bei den Einzelnen. Es wird wie in \cite{descObjbyAtr} auch versucht das Lernen des korrelierenden Attributs zu vermeiden und die daraus resultierenden Klassifzierer werden verwendet, um neue Attributwerte zu einem Attribut zu lernen. Die Ergebnisse zeigen, dass bei geringen Mengen an Trainingsdaten, die reduzierten Klassifizierer besser dazu geeignet sind neue Werte zu lernen. Die Autoren kommen damit zu folgender Aussage, die für die Perzeption von Robotern und das Trainieren von \glspl{kb} von Bedeutung ist: 
\begin{quote}
\glqq We believe that the capability to learn from smaller sets
of examples will be particularly important in the context of
teaching robots about objects and attributes.\grqq
\end{quote} \par

\cite{pronobis1} führen Objektattribute, das Aussehen von Räumen, eine Topologische Struktur sowie menschliche Eingaben zusammen, um einem Roboter zu ermöglichen, sich in einer Umgebung zurechtzufinden und mit ihr und Menschen zu interagieren. Die einzelnen Informationen sind dabei von verschiedener Natur und in einer \gls{kb} hinterlegt. Diese besteht aus 4 Schichten: 
\begin{enumerate}
	\item \textit{kategorische Schicht:} enthält Modelle von Objekten, Räumen und Orientierungspunkten.
	\item \textit{konzeptuelle Schicht:} Commonsense Wissen und eine Taxonomie von räumlichen Konzepten. Hier ist das Wissen abgespeichert, dass Milch in der Küche ist. Die Darstellung erfolgt in relationalen Beziehungen: \textit{Küche hat-objekt Milch}.
	\item \textit{Eigenschaften des Raums:} ein Ort bekommt Attribute, um ihn zu beschreiben.
	\item \textit{konzeptuelle Map} Dies ist die \gls{kb} in Form eines graphischen Modells und erlaubt damit das Schlussfolgern über unbekannte Räume. 
\end{enumerate}
In einem Experiment sollte der Roboter sich nun in einer Büroumgebung zurechtfinden, also aus seiner Wahrnehmung mit Hilfe seiner Reasoning-Komponente die Räume identifizieren. Dazu wurden Modelle aus ähnlichen Räumen trainiert und dann mit unbekannten Räumen getestet. Während des Tests sammelte er Informationen über Form, Größe, Aussehen und Objekte innerhalb seiner Umgebung und versuchte über die Existenz abwesender Objekte mit Hilfe anderer Informationen zu schlussfolgern. Dabei konnten viele Räume korrekt identifiziert werden, was für das Zusammenspiel verschiedener Informationsquellen beim Schlussfolgern über Unbekanntes spricht.   
\todo{Oder in KB/Reasoning?}  \par


Eine Erweiterung des \robosherlock-\glspl{framework} (siehe Kapitel \ref{sec:robosherlock} auf S.\pageref{sec:robosherlock}) zur Erkennnung von Objekten und das Schlussfolgern über die wahrgenommene Szene wird in \cite{pr2looking} vorgestellt. Dabei wird Attribut-basierte Objekterkennung benutzt, um die wahrgenommen Eigenschaften der Objekte zu beschreiben. In einem ersten Schritt werden Objekthypothesen in der Szene aufgestellt, zu denen spezialisierten Experten Informationen extrahieren. Diese umfassen die Attribute: Farbe, Größe, Form, Text und Logo. Mit diesem Wissen wird eine gemeinsame Wahrscheinlichkeitsverteilung in Form eines \gls{mln} trainiert. Aus einer wahrgenommenen Szene kann nun Wissen geschlussfolgert werden, indem die Szene als Evidenz zu einer Anfrage benutzt wird. In den Experimenten wurden 21 Objektkategorien in 50 Szenen mit jeweils 5-10 Objekten pro Szene erstellt und damit das Vorgehen getestet. Zusätzlich wurden die Szenen in 4 Küchenszenarien, Frühstück, Mittagessen, Blick in den Kühlschrank und einen Schrank, eingeteilt. Jede Szene wurde manuell mit dem Wissen, in welchem Szenario man sich befindet angereichert, um so Aufgaben-spezifisches Wissen zu modellieren. Die Grundlegende Anfrage bezog sich auf die Kategorie der Objekte, es wurde also aus der wahrgenommen Szene mit dem \gls{mln} inferiert, um welche Objekte es sich wohl handelt. Das Verfahren erreicht eine hohe Erkennungsrate, was vor allem auf das Zusammenspiel der einzelnen Experten als Ensemble, sie ergänzen sich gegenseitig, und die Eigenschaften von \glspl{mln}, über alle Objekte in der Szene gleichzeitig schlussfolgern und das gemeinsame Auftreten von Objekten innerhalb einer Szene modellieren zu können, zurückgeführt wird.

\section{Virtuelle Realität}

\cite{brooks} \newline
\begin{quote}
\glqq I define a virtual reality experience as any in which the user is efectively immersed in a responsive virtual world. This implies user dynamic control of viewpoint.\grqq
\end{quote}
Auflistung von Technologien die für VR benötig sind - veraltet. Meiste Dinge davon mittlerweile gut/gelöst\todo{???} \par

\cite{burger1995} \newline
autonome Roboter in veränderlichen Ungebungen müssen robust/reliable arbeiten. Verfirkation der komplexen Systeme schwer. Testing nur mit pre-recorded Bildern. Roboter selbst nicht beim Design/develpoment anwesend... Lernende Systeme brauchen fiel Training, wofür häufig testdaten mit Groundtruth und realistische Testumgebungen fehlen -> Bottleneck in Designprozess. \newline
Darum wird große Zahl an Testdaten benötigt und eine eine Testumgebung, um das ganze Perzeptionssystem vor dem Deployment zu testen. Darum Simulation benutzten. \newline
Simulationen :
\begin{itemize}
	\item ausfürhliches Testen in unterschiedlichen Umgebungen
	\item GT informationen
	\item autonomes Lernen ohne menschliche Intervention
	\item realistisches Testen in kompletten Umgebungen
	\item verringerte Kosten 
\end{itemize}
über Objekterknnung/Vision in complex Env: große Zahl an Trainigsdaten benötigt, was Lernen time-consuming macht \todo{kann man als cite für andere stellen benutzen}. SImulation dafür gut:
\begin{itemize}
	\item erstellung vieler Trainigsdaten
	\item learning ohne Mensch, da GT vorhanden
	\item Closedloop (or ''exploratory'') learning, where specific, critical training data are generated in response to the learning progress, can be performed if needed.
\end{itemize}

\cite{heisele} \newline
In the following, we briefly discuss the pros and cons of 3D models for view-based object
recognition:
+ Large numbers of training and test images can be generated fully automatically.
+ Full control over image generation parameters, including internal and external camera
parameters, illumination, composition of the scene, and animation of the scene.
+ Ground truth for the location, scale, and orientation of each object. In video sequences,
the frame rate, the camera motion, and the motion of objects are known.
- Lack of 3D models and 3D scenes. The situation might improve with widespread
use of low-cost 3D scanners and the availability of models designed for commercial
purposes (e.g., computer games, movies, Google earth 3D) and by a fast growing
community of hobbyist modelers.
- Lack of realism of synthetic data. For recognition systems that operate at low pixel
resolutions, the quality of synthetic image data is usually sufficient. Even at high image
resolutions, free rendering software is capable of producing photorealistic
results. However, modeling and rendering of photorealistic scenes can be time consuming
processes.
- Large, accurately annotated data sets might not be necessary; humans are capable of
learning to recognize new objects from a small number of images. In order to build
computer vision systems with this capability, we have to better understand how humans
use prior knowledge, we have to be able to implement complex visual processing
steps (such as segmentation and shape from shading), and we have to understand
the top-down learning mechanisms in the visual cortex.


\cite{imitationLearning} \newline
weg von pereption, aber mit VR(virtual environment) und Roboters. \newline
Roboter sollten sich wie Menschen bewegen, damit einfacher mit ihnen zu leben. Das ist schwierig zu lernen. Selbst einfache Aufgaben schwierig. Um Aufgaben zu bewältigen, Wissen wie? muss durch Acquisiton und Lerenn beigebracht werden. Menschen sind Experten bei solchen Augaben. Also sie machen vor, tracked + analyzed, darausModell für Roboter. Bracuht auch viele Daten- langweilig und Zeitintensiv für Menschen. \newline
Stattdessen in VR benutzen. User benutzt simulierte Umgebung, daruas Modelle bauen. VR Vorteile: fully observable, no real-worl constraints, keine saftey issues. Spiele können Motivation erhöhen, umso Anzahl an Demonstrationen zu erhöhen womit mehr Motion Daten zur Vefügung. \newline
Games um Leute zu motivieren high quality Daten zu geben und einfache Aufgaben zu bewältigen. Aquisition mit Game. 3d free-hand contro Tower defense 

\section{MLN oder sowas}

\section{Ziel der Arbeit}
\label{sec:goal}

\todo{Object Recognitiin mit 3D-Models hat schöne Liste für und gegen 3D-Model Training, vllt hier einflißen lassne.}

Untersuche ob die Perzeption an diesen Daten und das Reasoning über ein KB mit den Daten gute Ergebnisse liefert. Dann automatisierung von Testdaten ersteelung möglich. 