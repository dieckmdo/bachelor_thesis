\graphicspath{{./images/}}      
\def\CHAPTERONE{./chapters/Chapter-1} 

\chapter{Umsetzung}
\label{chap:implementation}
%	\input{\CHAPTERONE /motivation}

In diesem Kapitel wird beschrieben, wie die einzelnen Arbeitsschritte umgesetzt wurden. Zuerst wird aufgezeigt, wie die Bildern in der \unreal erstellt wurden. Zur Vereinfachung werden diese  im Folgenden \textit{Unreal Bilder} genannt. Danach wird die verwendete \gls{ae} erläutert, die die Bilder annotiert. \todo{was kommt dann?}

\section{Erstellen der Unreal Bilder}
\label{sec:takingpics}
Um Unreal Bilder zum Trainieren eines \gls{mln} zur Verfügung zu haben, müssen diese in der \unreal erstellt werden. Dazu wurde die in Kapitel \todo{...} genannte Küchenumgebung benutzt. Hier wurden Objekte des täglichen Gebrauchs in der Küche manuell drapiert und mittels des \textit{URoboVision}-Plugins Bilder davon gemacht und an \robosherlock gesendet. Eine solche Anordnung von Objekten in der Küche wird \textit{Szene} genannt. Von jeder Szene wurden Bilder aus mehreren festgelegten Blickwinkeln mittels der \textit{SpawnBox} erstellt. \par 

\textbf{URoboVisionPlugin:} Das Plugin bietet einen in die Szene einfügbaren \todo{RCameraActor??}. Die verwendeten Parameter sind \todo{hier einfügren}. \par 

\textbf{Aufnahme aus verschiedene Blickwinkeln:} Die veränderte \textit{SpawnBox} besitzt eine \textit{BoxVolume} aus der vorherigen Iteration. Sie wurde beibehalten, da sie eine visuelle Markierung bietet, wo die Objekte drapiert werden sollten, damit sie im späteren Bild noch zu sehen sind und nicht außerhalb den Blickfeldes der Kamera liegen. Der Mittelpunkt der Box dient als Rotationspunkt, um den eine referenzierte Kamera basierend auf einigen Parametern rotiert wird. Die Kamera wird dabei nach einer \todo{bestimmten Zeit} in einem einstellbaren Abstand $x$-mal um einen einen einstellbaren Winkel von ihrem aktuellen Standort um den Mittelpunkt rotiert. Der Startpunkt der Kamera wird über den Mittelpunkt der Box ermittelt und befindet sich \textit{vor} der Box, also in \todo{x richtung 0}, sodass die Kamera nur Bilder abhängig von den Parametern der \textit{SpawnBox} macht, und nicht irgendwo anders im Raum. Die Höhe der Kamera kann ebenfalls eingestellt werden, wobei der Mittelpunkt der Box als ebenerdig, also die Höhe 0, angesehen wird. Die Kamera kann auch auf zwei unterschiedlichen Höhen Operieren, wobei der Winkel und die Rotationsschritte unabhängig voneinander eingestellt werden können. Um die Bedienung mit der Box als Markierung zu erleichtern, wurde es außerdem möglich gemacht, die Kamera nicht vor der Box starten zu lassen, sondern diesen Ort als Mittelpunkt der Rotation anzusehen. Werden zum Beispiel drei Rotationsschritte mit einem Winkel von $30^\circ$ erwünscht, wären die Standorte der Kamera bei der normalen Einstellung \todo{vor und jeweils $30^\circ$ weiter links  (abhängig von richtung siehe oben?!?)} von der Box. Bei der anderen Einstellung würde die Kamera $30^\circ$ rechts vom normalen Startpunkt beginnen, dann auf den eigentlichen Startpunkt kommen und dann $30^\circ$ nach links schwenken. So wird es einfacher Blickwinkel zu generieren, die dem Fahren und Umsehen eines Roboters ähnlicher sind. \newline
\todo{Yes/No? Behalte als refernz für andere erstmal}Die einzelnen Parameter und ihre Funktion sowie die gewählten Werte werden in \ref{tab:spawnboxParams} kurz zusammengefasst.
\begin{table}
\label{tab:spawnboxParams}
\begin{tabular}{lll}
RotationSteps & Wie häufig die Kameraposition verändert werden soll. & 12 \\
Winkel & Der Winkel, um den die Kameraposition verändert werden soll. & 4 \\
foo &  & \\
bar &  & \\
\end{tabular}
\caption{Parameter der \textit{RSpawnbox}}
\end{table}

\todo{den Algorithmus der SpawnBox}
\begin{algorithm}[H]
\begin{algorithmic}
\FOR{$i=0$ to $10$}
\STATE carry out some processing
\ENDFOR
\end{algorithmic}
\caption{Blickwinkelberechnung der RSpawnBox}
\end{algorithm}

 \par

Um die Unreal Bilder zu erstellen, wurde die Parameter so eingestellt, dass die Kamera auf einer Höhe von ... und ... um die Szene rotiert. Es werden dabei 5 Bilder gemacht, 3 aus Höhe1 zwei aus Höhe2. etc.    Abspeicherung in MongoDB.

\section{Analysis Engine}
\label{sec:analysisengine}
Die Unreal Bilder können  von einer Perzeptionspipeline in \robosherlock annotiert werden und so die Objektattribute als logische Prädikate zum Training eines \gls{mln} erhalten werden. Dazu werden die Unreal Bilder aus der Datenbank herausgelesen und als Eingabe benutzt. Auf diesen werden zuerst eine Reihe von \textit{hyptheses generators} zur Segmentierung laufen lassen mit denen die Objekthypothesen erstellt werden. Dabei werden jedoch auch Objekthypothesen zu Formen und Teilen des Bildes erstellt, die keine Objekte sind, wie die Front eines Schrankes, oder keine für die Arbeit relevanten Objekte, wie die Herdplatte. Der \textit{UnrealGTAnnotator} filtert diese \textit{Cluster} nun heraus und annotiert zu den übrig gebliebenen die \gls{gt}. Die gebliebenen Hypothesen werden von den Annotatoren auf Eigenschaften analysiert und die Ergebnisse als Annotationen in der \gls{cas} gespeichert. In einem letzten Schritt werden die Annotationen von dem \textit{MLNInferencer} als logische Prädikate in einer Textdatei ausgeben. \par 

Im Folgenden wird der \textit{UnrealGTAnnotator} und seine Implementierung erläutert. Danach werden die Annotatoren der Perzeptionspipeline vorgestellt. Tabelle \ref{tab:annotators} bietet eine kurze Übersicht über die Annotatoren, die resultierenden logischen Prädikate und die möglichen Werte.

\subsection{UnrealGTAnnotator}


\subsection{Annotatoren}

Im Folgenden werden die Annotatoren vorgestellt. Sie analysieren die Objekthypothesen auf Eingenschaften. \todo{Warum shape zweimal?} Sie sind in einer Perzeptionspipeline zusammengefasst und kommen auch in der Reihenfolge zum Einsatz, in der sie hier vorgestellt werden. Tabelle \ref{tab:annotators} bietet eine kurze Übersicht.

\begin{table}
\begin{tabularx}{\textwidth}{lXX}
\textbf{Annotator}				& \textbf{MLN-Prädikat}	& \textbf{Werte}	\\ \hline 
Cluster3DGeometryAnnotator		& size(cluster, size)	&		\\ \hline 
PrimitiveShapeAnnotator			& shape(cluster, shape)	&		\\ \hline 
ClusterColorHistogramCalculator  & color(cluster, color)	& yellow, red, blue, green, white, black \\ \hline 
GogglesAnnotator				& logo(cluster, logo) \newline text(cluster, text) \newline texture(cluster, t) \newline \todo{???mehr}	& 	variabel	\\ \hline 
CaffeAnnotator					& --					& --	\\ \hline 
PCLFeatureExtractor				& --					& --	\\ \hline 
RfAnnotator						& \todo{?}				&		\\ \hline 
SVMAnnotator					&\todo{?}				&		\\ \hline 
\end{tabularx}
\caption{Ein Kurzüberblick über die Annotatoren und die korrespondieren logischen Prädikate für das \gls{mln}.}
\label{tab:annotators}
\end{table}

\subsubsection{Cluster3DGeometryAnnotator}

\subsubsection{PrimitiveShapeAnnotator} 
   
\subsubsection{ClusterColorHistogramCalculator}

\subsubsection{GogglesAnnotator}

\subsubsection{CaffeAnnotator}

\subsubsection{PCLFeatureExtractor}

\subsubsection{RfAnnotator}

\subsubsection{SVMAnnotator}

\subsubsection{MLNInferencer}
Der \textit{MLNInferencer} liest die Annotationen aus der \gls{cas} aus gibt sie als logische Prädikate in einer Textdatei aus. Diese Dateien werden Datenbank genannt und bilden die Trainingsdaten für ein \gls{mln}. Der \textit{MLNInferencer} kann die Prädikate sowohl in \gls{fo} ausgeben, als auch in \textit{Fuzzy-Logik}. Dabei werden für jedes Prädikat die Konfidenzen der Annotatoren mit ausgegeben.

\begin{algorithm}[H]
\begin{lstlisting}
if(isCodeIsFancy){
// blah
}
\end{lstlisting}
 \caption{How to write algorithms}
\end{algorithm}
