\graphicspath{{./images/}}      
\def\CHAPTERONE{./chapters/Chapter-1} 

\chapter{Experimente}
\label{chap:experiments}
%	\input{\CHAPTERONE /motivation}
\glsresetall
\todo{überarbeiten}Im folgenden Kapitel werden die Experimente beschrieben, die im Rahmen dieser Arbeit durchgeführt wurden, um die Objekterkennung in fotorealistischen Bildern zu evaluieren.
In einem ersten Experiment wird ein \gls{klassifikator} trainiert und mit den Unreal-Bildern getestet. 

\section{Erklärung und Analyse der Datensätze}

In diesem Abschnitt wird beschrieben, wie sich Szenen der Unreal-Bilder zusammensetzen und die Häufigkeit des Auftretens der einzelnen Klassen und Instanzen analysiert. Da das Ziel ist, ein \gls{mln} mit den Unreal-Bildern zu trainieren und mit diesem echte Bilder zu klassifizieren, wurde auch ein Satz echter Bilder erstellt. Dieser wird hier auch erläutert.

\subsection{Unreal-Bilder}  
Für die Experimente wurden mit dem in Kapitel \ref{sec:takingpics} vorgestellten Verfahren 114 Szenen erstellt und von ihnen fünf Bilder aufgenommen (Beispielszene auf Seite \pageref{fig:exampleScene}, Abblidung \ref{fig:exampleScene}). Jede Szene enthält zwischen 2 und 5 Objekten. Jedes Objekt ist mindestens einem der folgenden Szenarios zugeordnet: \textit{breakfast}, \textit{cooking} oder \textit{fridge}. Die Zuordnung ist in Tabelle \ref{tab:objects} einzusehen. Eine Szene enthält nur Objekte eines Szenarios. Dies ist echten Bedingungen nachempfunden, denn in der Regel finden sich im Kühlschrank keine Gabeln, Milch allerdings schon, die auch auf einem Frühstückstisch zu finden sein kann. Es wurden jeweils 50 \textit{breakfast} und \textit{cooking} Szenen sowie 14 \textit{fridge} Szenen erstellt. Insgesamt stehen so \textbf{570 Unreal-Bilder} zur Verfügung. \par

In Abbildung \ref{fig:Unreal-Images_analysis} ist die Verteilung der Objekte in den gesamten Unreal-Bildern zu sehen. Ein Großteil der Objektinstanzen kommt in einer ähnlichen Anzahl in den Bildern vor. Objekte, die mehreren Szenarien zugeordnet sind, wie der AlbiHimbeerJuice dem \textit{breakfast} und \textit{cooking}, jedoch bis zu doppelt so häufig. Dies ist durch die Art der Erstellung der Unreal-Bilder zu erklären: Es wurde versucht pro Szenario eine gleichmäßige Verteilung zu erreichen. \newline
Die ungleiche Verteilung der Objektklassen ist bedingt durch die unterschiedliche Anzahl an Objektinstanzen pro Klasse. Es gibt nur eine ein Objekt, dass der Coffee Klasse angehört, jedoch vier Objekte die der Bowl Klasse angehören. \newline
Es wird dementsprechend von einem ungleich verteilten Datensatz ausgegangen. Bei der Berechnung der Fehlergrößen \gls{precision}, \gls{recall} und \gls{f1score} der Klassifikation des gesamten Datensatzes, wird deshalb der \textit{gewichtete} Durchschnitt der Fehlergrößen pro Instanz/Klasse verwendet. 

\begin{figure}
\centering
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[width=\textwidth]{img/chapter6/UnrealGTClass_analysis.png}
	\end{subfigure}
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[width=\textwidth]{img/chapter6/UnrealGTInstance_analysis.png}	
	\end{subfigure}
\caption[Verteilung der Objekte in den Unreal-Bildern]{Verteilung der Objektklassen und Instanzen in den gesamten Unreal-Bildern.}
\label{fig:Unreal-Images_analysis}
\end{figure}



\subsection{Reale Bilder}
Mit dem PR2-Roboter des \gls{iai} wurden Bilder in der realen Küchenumgebung aufgenommen. Die verwendeten Objekte sind dabei die echten Gegenstücke zu den eingescannten Objekten. Unglücklicherweise stand zu dem Zeitpunkt der Aufnahme der LinuxCup und die YellowPlate nicht mehr zur Verfügung. Für die YellowPlate wurde stattdessen ein weißer Teller als Ersatz Objekt verwendet. Es wurden wieder 114 zufällig aufgebaute Szenen mit der gleichen Szenarien-Verteilung und den gleichen Bedingungen erstellt. Auf Grund des erhöhten Aufwandes bei der Aufnahme der Szenen, wurden von jeder Szene nur drei Bilder aufgenommen, womit insgesamt \textbf{342 reale Bilder} zur Verfügung stehen. Im Gegensatz zu den Unreal-Bildern sind die Blickwinkel nicht exakt die Gleichen für jede Szene, da der Roboter bei der Aufnahme manuell bewegt werden muss (eine Szene ist in Abbildung \ref{fig:exampleSceneReal} zu sehen).

\begin{figure}
\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[scale=.1]{img/chapter6/real1}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[scale=.1]{img/chapter6/real2}	
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[scale=.1]{img/chapter6/real3}	
	\end{subfigure}
\caption[Reale Bilder einer Szene]{Ein Bildersatz der realen Szenen.}
\label{fig:exampleSceneReal}
\end{figure}

Die Verteilung der Objektklassen und Instanzen in den realen Bildern, zu sehen in Abbildung \ref{fig:Real-Images_analysis}, zeigt die gleichen Eigenschaften, wie die Verteilung der Unreal-Bilder. Es wird auch hier von einem ungleich verteilten Datensatz ausgegangen. 

\begin{figure}
\centering
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[width=\textwidth]{img/chapter6/RealGTClass_analysis.png}
	\end{subfigure}
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[width=\textwidth]{img/chapter6/RealGTInstance_analysis.png}	
	\end{subfigure}
\caption[Verteilung der Objekte in den realen Bildern]{Verteilung der Objektklassen und Instanzen in den gesamten realen Bildern.}
\label{fig:Real-Images_analysis}
\end{figure}

\section{Klassifizierung mit Klassifikatoren}
\label{sec:classificationExperiment}
Die zuvor erstellten Unreal-Bilder, werden in ersten Experimenten über Merkmale klassifiziert. Dazu wird ein Annotator aus dem \robosherlock-Paket \textit{rs\_addons} als \gls{klassifikator} mit echten Bildern der verwendeten Objekte trainiert. Mehr zu dem Paket und den \glspl{klassifikator} ist in \ref{sec:classifiers} auf Seite \pageref{sec:classifiers} zu finden. \par

Als \glspl{klassifikator} werden der \texttt{RFAnnotator} und der \texttt{SVMAnnotator} verwendet, die jeweils auf einer \gls{rf} oder \gls{svm} Implementierung basieren. Die Objekte sollten in zwei Versuchsreihen entweder den Objektklassen oder den Instanznamen zugeordnet werden. Die Bilder der echten Objekte wurden den Klassen/Instanzen zugeordnet, bevor ein \gls{klassifikator} damit trainiert wurde. Da es sich um Annotatoren für \robosherlock handelt, wurde eine \gls{ae} mit einigen \textit{hyotheses generators}, dem CaffeAnnotator zum Extrahieren der Merkmale (siehe Kap. \ref{sec:caffeAnno}, S.\pageref{sec:caffeAnno}), dem UnrealGTAnnotator zum herausziehen der \gls{gt} aus den Unreal-Bildern und dem \gls{klassifikator} erstellt. Um die Ergebnisse später verarbeiten zu können werden sie mit dem MLNInference (siehe Kap. \ref{sec:mlnInferencer}, S. \pageref{sec:mlnInferencer}) als logische Prädikate ausgegeben. Die so erhaltene Klassifizierung wird für jedes Objekt mit der \gls{gt} verglichen, um so die Erkennungsrate des \gls{klassifikator}s zu ermitteln.


\subsection{Objektklassen}
Die Ergebnisse des \texttt{RFAnnotators} sind in Abbildung \ref{fig:RFClassifierGTClass_confMatrix} und Tabelle \ref{tab:RFClassifierGTClass_metrics} abgebildet. Während nahezu alle Objekte eine \gls{accuracy} über 90\% aufweisen, zeigen \gls{precision}, \gls{recall} und \gls{f1score}, dass der \texttt{RFAnnotator} große Schwierigkeiten hat, visuell ähnliche Objekte auseinander zu halten. Besonders häufig werden Objekte fälschlicherweise als Milch (Milk) und Salz (TableSalt)  eingeordnet. Dies ist auf die box-artige Form und die Ähnlichkeit der Farbe vieler Objekte zu der Milch und dem Salz zurückzuführen, welche insgesamt in größerer Zahl in den Daten vorkommen als andere Objektklassen. Des weiteren hat der \gls{klassifikator} Schwierigkeiten das Geschirr auseinander zu halten, was auf die Größe dessen zurückzuführen ist. Besonders schlecht schneidet der Pfannenwender (Spatula) ab, der nicht einmal korrekt erkannt wurde. Mit einem \gls{f1score} über 0,8 schneiden die Objektklassen Buttermilch (Buttermilk), Kaffee (Coffee), Becher (Cup) und PancakeMix am besten ab. Diese Klassen unterscheiden sich in Form (Buttermilk, PancakeMix) und Farbe (Coffee, Cup) stark von den anderen Objekten.  

\begin{figure}
	\includegraphics[scale=.4]{img/chapter6/RFClassifierGTClass.png}
\caption[Konfusionsmatrix der Klassifizierung der Objektklassen durch den RFAnnotator]{Die Konfusionsmatrix für die Objektklassen Klassifikation der Unreal-Bilder durch den mit echten Bildern trainierten \texttt{RFAnnotator}.}
\label{fig:RFClassifierGTClass_confMatrix}
\end{figure}

\begin{table}
\rowcolors{1}{}{lightgray}
\begin{tabularx}{\textwidth}{Xllll}
\textbf{Objekt}	& \textbf{\gls{accuracy}} & \textbf{\gls{precision}}	& \textbf{\gls{recall}}	& \textbf{\gls{f1score}} \\ \hline
Bowl & 0.91 & 0.61 & 0.81 & 0.69 \\  
BreakfastCereal & 0.92 & 0.9 & 0.61 & 0.72 \\  
Buttermilk & 1.0 & 1.0 & 0.98 & 0.99 \\  
Coffee & 0.99 & 0.96 & 0.86 & 0.91 \\  
Cup & 0.97 & 0.88 & 0.77 & 0.82 \\  
DinnerPlate & 0.93 & 0.73 & 0.53 & 0.61 \\  
DrinkingBottle & 0.97 & 1.0 & 0.31 & 0.47 \\  
DrinkingMug & 0.93 & 0.49 & 0.89 & 0.63 \\  
Fork & 0.93 & 0.47 & 0.68 & 0.55 \\  
Juice & 0.97 & 1.0 & 0.46 & 0.63 \\  
Knife & 0.91 & 0.31 & 0.43 & 0.36 \\  
Milk & 0.83 & 0.51 & 0.92 & 0.66 \\  
PancakeMaker & 0.96 & 0.88 & 0.13 & 0.22 \\  
PancakeMix & 0.99 & 0.9 & 0.8 & 0.84 \\  
Rice & 0.97 & 0.58 & 0.16 & 0.25 \\  
Spatula & 0.96 & 0.0 & 0.0 & 0.0 \\  
Spoon & 0.9 & 0.42 & 0.35 & 0.38 \\  
TableSalt & 0.88 & 0.41 & 0.78 & 0.54 \\  
Tea-Iced & 0.9 & 1.0 & 0.05 & 0.09 \\  
TomatoSauce & 0.94 & 0.59 & 0.36 & 0.44 \\  \hline
\textbf{Gesamt}		&	\textbf{0.6}   &	\textbf{0.67}  & \textbf{0.6}     &  \textbf{0.57}     \\
\end{tabularx}
\caption[Objektklassen-spezifische Kenngrößen des RFAnnotators]{Kenngrößen für die einzelnen Objekte aus der Objektklassen Klassifizierung der Unreal-Bilder mit dem \texttt{RFAnnotator}.}
\label{tab:RFClassifierGTClass_metrics}
\end{table}

Die Ergebnisse des \texttt{SVMAnnotators} sind in Abbildung  \ref{fig:SVMClassifierGTClass_confMatrix} zu sehen. Insgesamt sind die Ergebnisse gegenüber dem \texttt{RFAnnotator} etwas besser, da der \texttt{SVMAnnotator} mit 62\% eine etwas höhere \gls{accuracy} als 60\% aufweist. Jedoch werden vom \texttt{SVMAnnotator} neben dem Pfannenwender auch der PancakeMaker gar nicht richtig eingeordnet und auch die Flasche (DrinkingBottle) schneidet sehr schlecht ab. Der SVMAnnotator ist insgesamt bei einigen Klassen etwas zuverlässiger, hat jedoch auch einige größere Fehlerquellen, was insgesamt zu dem gleichen \gls{f1score} des \texttt{RFAnnotators} von 57\% führt.  

\begin{figure}
	\includegraphics[scale=.4]{img/chapter6/SVMClassifierGTClass.png}
\caption[Konfusionsmatrix der Klassifizierung der Objektklassen durch den SVMAnnotator]{Die Konfusionsmatrix für die Objektklassen Klassifikation der Unreal-Bilder durch den mit echten Bildern trainierten \texttt{SVMAnnotator}.}
\label{fig:SVMClassifierGTClass_confMatrix}
\end{figure}

\begin{table}
\rowcolors{1}{}{lightgray}
\begin{tabularx}{\textwidth}{Xllll}
\textbf{Objekt}	& \textbf{\gls{accuracy}} & \textbf{\gls{precision}}	& \textbf{\gls{recall}}	& \textbf{\gls{f1score}} \\ \hline
Bowl & 0.89 & 0.53 & 0.99 & 0.69 \\  
BreakfastCereal & 0.92 & 1.0 & 0.53 & 0.69 \\  
Buttermilk & 1.0 & 1.0 & 1.0 & 1.0 \\  
Coffee & 1.0 & 1.0 & 1.0 & 1.0 \\  
Cup & 0.99 & 1.0 & 0.86 & 0.93 \\  
DinnerPlate & 0.91 & 1.0 & 0.12 & 0.21 \\  
DrinkingBottle & 0.97 & 1.0 & 0.04 & 0.09 \\  
DrinkingMug & 0.94 & 0.5 & 0.68 & 0.57 \\  
Fork & 0.97 & 0.73 & 0.7 & 0.71 \\  
Juice & 0.99 & 1.0 & 0.86 & 0.93 \\  
Knife & 0.93 & 0.44 & 0.83 & 0.57 \\  
Milk & 0.88 & 0.6 & 0.98 & 0.74 \\  
PancakeMaker & 0.96 & 0.0 & 0.0 & 0.0 \\  
PancakeMix & 1.0 & 0.94 & 1.0 & 0.97 \\  
Rice & 0.97 & 0.77 & 0.22 & 0.34 \\  
Spatula & 0.96 & 0.0 & 0.0 & 0.0 \\  
Spoon & 0.9 & 0.42 & 0.4 & 0.41 \\  
TableSalt & 0.85 & 0.32 & 0.72 & 0.45 \\  
Tea-Iced & 0.91 & 0.83 & 0.04 & 0.07 \\  
TomatoSauce & 0.96 & 0.71 & 0.65 & 0.68 \\    \hline
\textbf{Gesamt}		&	\textbf{0.62}   &	\textbf{0.7}  & \textbf{0.62}     &  \textbf{0.57}     \\
\end{tabularx}
\caption[Objektklassen-spezifische Kenngrößen des SVMAnnotators]{Kenngrößen für die einzelnen Objekte aus der Objektklassen Klassifizierung der Unreal-Bilder mit dem \texttt{SVMAnnotator}.}
\label{tab:SVMClassifierGTClass_metrics}
\end{table}

\subsection{Instanznamen}

Die Ergebnisse der Klassifikation der Instanzen des \texttt{RFAnnotators} ist in Abbildung \ref{fig:RFClassifierGTInstance_confMatrix} und Tabelle \ref{tab:RFClassifierGTInstance_metrics} zu finden. Grundsätzlich lässt sich sagen, das die Erkennungsrate nahezu identisch ist. Die Fehler bei den Klassen wurden auf die Instanzen übertragen und es ist nun an den Instanzen klar zu erkennen, wo die Fehler passieren. Beispielsweise wird der SlottedSpatula in der Hälfte der der Fälle für den LargeGreySpoon gehalten. Da beide sich ähnlich sehen, ist dies keine Überraschung, zeigt jedoch auch, dass die Ähnlichkeit für den \texttt{RFAnnotator} zu groß ist. In den anderen Fällen ist er einem der blauen Geschirrteile zugeordnet, nicht den Roten. \newline
Interessanterweise werden sowohl die BluePlasticBowl als auch die WhiteCeramicIkeaBowl nicht einmal richtig erkannt. Erstere wird häufig für Letztere gehalten. Die IkeaBowl stattdessen für die EdekaRedBowl. Dies fiel bei der Klassenklassifikation noch nicht auf, da alle unter Bowl zusammengefasst sind und so trotzdem richtig eingeordnet wurden.   

\begin{figure}
	\includegraphics[scale=.4]{img/chapter6/RFClassifierGTInstance.png}
\caption[Konfusionsmatrix der Klassifizierung der Objektinstanzen durch den RFAnnotator]{Die Konfusionsmatrix für die Objektinstanzen Klassifikation der Unreal-Bilder durch den mit echten Bildern trainierten \texttt{RFAnnotator}.}
\label{fig:RFClassifierGTInstance_confMatrix}
\end{figure}

\begin{table}
\rowcolors{1}{}{lightgray}
\begin{tabularx}{\textwidth}{Xllll}
\textbf{Objekt}	& \textbf{\gls{accuracy}} & \textbf{\gls{precision}}	& \textbf{\gls{recall}}	& \textbf{\gls{f1score}} \\ \hline
AlbiHimbeerJuice & 0.99 & 0.89 & 0.91 & 0.9 \\  
BlueCeramicIkeaMug & 0.95 & 0.32 & 0.63 & 0.42 \\  
BlueMetalPlateWhiteSpeckles & 0.99 & 0.88 & 0.81 & 0.84 \\  
BluePlasticBowl & 0.96 & 0.0 & 0.0 & 0.0 \\  
BluePlasticFork & 0.96 & 0.36 & 0.38 & 0.37 \\  
BluePlasticKnife & 0.93 & 0.23 & 0.53 & 0.32 \\  
BluePlasticSpoon & 0.96 & 0.14 & 0.05 & 0.07 \\  
CupEcoOrange & 1.0 & 0.97 & 0.95 & 0.96 \\  
EdekaRedBowl & 0.93 & 0.31 & 1.0 & 0.47 \\  
ElBrygCoffee & 1.0 & 0.92 & 0.98 & 0.95 \\  
JaMilch & 0.97 & 0.67 & 0.98 & 0.8 \\  
JodSalz & 0.96 & 0.44 & 0.64 & 0.52 \\  
KelloggsCornFlakes & 0.98 & 0.63 & 0.55 & 0.59 \\  
KelloggsToppasMini & 0.97 & 0.82 & 0.23 & 0.36 \\  
KnusperSchokoKeks & 0.99 & 0.96 & 0.67 & 0.79 \\  
KoellnMuesliKnusperHonigNuss & 0.99 & 0.82 & 0.89 & 0.85 \\  
LargeGreySpoon & 0.96 & 0.13 & 0.09 & 0.11 \\  
LinuxCup & 0.98 & 0.6 & 0.88 & 0.71 \\  
LionCerealBox & 0.99 & 0.95 & 0.88 & 0.91 \\  
MarkenSalz & 0.98 & 0.57 & 0.95 & 0.72 \\  
MeerSalz & 0.99 & 0.94 & 0.71 & 0.81 \\  
MondaminPancakeMix & 0.99 & 0.76 & 0.93 & 0.84 \\  
NesquikCereal & 0.97 & 0.78 & 0.18 & 0.29 \\  
PfannerGruneIcetea & 0.97 & 0.86 & 0.38 & 0.53 \\  
PfannerPfirsichIcetea & 0.97 & 0.86 & 0.54 & 0.67 \\  
RedMetalBowlWhiteSpeckles & 0.98 & 0.8 & 0.53 & 0.64 \\  
RedMetalCupWhiteSpeckles & 0.98 & 0.58 & 0.97 & 0.73 \\  
RedMetalPlateWhiteSpeckles & 0.99 & 1.0 & 0.82 & 0.9 \\  
RedPlasticFork & 0.98 & 0.69 & 0.55 & 0.61 \\  
RedPlasticKnife & 0.95 & 0.34 & 0.89 & 0.49 \\  
RedPlasticSpoon & 0.96 & 0.16 & 0.08 & 0.11 \\  
ReineButterMilch & 0.98 & 0.76 & 1.0 & 0.86 \\  
SeverinPancakeMaker & 0.96 & 0.62 & 0.15 & 0.24 \\  
SiggBottle & 0.98 & 1.0 & 0.38 & 0.55 \\  
SlottedSpatula & 0.96 & 0.17 & 0.02 & 0.04 \\  
SojaMilch & 0.98 & 0.87 & 0.62 & 0.72 \\  
SpitzenReis & 0.97 & 0.53 & 0.42 & 0.47 \\  
TomatoAlGustoBasilikum & 0.97 & 0.53 & 0.79 & 0.64 \\  
TomatoSauceOroDiParma & 0.98 & 0.96 & 0.53 & 0.69 \\  
VollMilch & 0.95 & 0.48 & 0.86 & 0.62 \\  
WeideMilchSmall & 0.98 & 0.7 & 0.91 & 0.79 \\  
WhiteCeramicIkeaBowl & 0.93 & 0.0 & 0.0 & 0.0 \\  
YellowCeramicPlate & 0.96 & 0.0 & 0.0 & 0.0 \\   \hline
\textbf{Gesamt}		&	\textbf{0.6}   &	\textbf{0.62}  & \textbf{0.6}     &  \textbf{0.58}     \\
\end{tabularx}
\caption[Objektinstanzen-spezifische Kenngrößen des RFAnnotators]{Kenngrößen für die einzelnen Objekte aus der Objektinstanzen Klassifizierung der Unreal-Bilder mit dem \texttt{RFAnnotator}.}
\label{tab:RFClassifierGTInstance_metrics}
\end{table}

Der \texttt{SVMAnnotator} schneidet bei den Instanzen etwas besser ab als bei den Klassen. Dementsprechend auch besser als der \texttt{RFAnnotator} bei den Instanzen. Auch hier sind die Fehler aus der Klassenklassifikation übertragbar. Die SiggBottle wird selten erkannt oder als ReineButterMilch oder MondaminPancakeMix eingeordnet. Zu Verwirrung scheint es hier zu kommen, da sie alle rund sind und eine ähnlich helle Farbe aufweisen. Insgesamt sind die Fehlerquellen jedoch wieder nahezu deckungsgleich mit dem \texttt{RFAnnotator}.

\begin{figure}
	\includegraphics[scale=.4]{img/chapter6/SVMClassifierGTInstance.png}
\caption[Konfusionsmatrix der Klassifizierung der Objektinstanzen durch den SVMAnnotator]{Die Konfusionsmatrix für die Objektinstanzen Klassifikation der Unreal-Bilder durch den mit echten Bildern trainierten \texttt{SVMAnnotator}.}
\label{fig:SVMClassifierGTInstance_confMatrix}
\end{figure}

\begin{table}
\rowcolors{1}{}{lightgray}
\begin{tabularx}{\textwidth}{Xllll}
\textbf{Objekt}	& \textbf{\gls{accuracy}} & \textbf{\gls{precision}}	& \textbf{\gls{recall}}	& \textbf{\gls{f1score}} \\ \hline
AlbiHimbeerJuice & 1.0 & 1.0 & 0.98 & 0.99 \\  
BlueCeramicIkeaMug & 0.96 & 0.4 & 0.8 & 0.53 \\  
BlueMetalPlateWhiteSpeckles & 1.0 & 0.98 & 0.96 & 0.97 \\  
BluePlasticBowl & 0.97 & 1.0 & 0.02 & 0.04 \\  
BluePlasticFork & 0.97 & 0.52 & 0.55 & 0.54 \\  
BluePlasticKnife & 0.94 & 0.28 & 0.63 & 0.38 \\  
BluePlasticSpoon & 0.96 & 0.0 & 0.0 & 0.0 \\  
CupEcoOrange & 1.0 & 1.0 & 1.0 & 1.0 \\  
EdekaRedBowl & 0.95 & 0.36 & 1.0 & 0.53 \\  
ElBrygCoffee & 1.0 & 1.0 & 1.0 & 1.0 \\  
JaMilch & 0.99 & 0.91 & 0.95 & 0.93 \\  
JodSalz & 0.98 & 0.71 & 0.64 & 0.68 \\  
KelloggsCornFlakes & 0.99 & 0.84 & 0.93 & 0.88 \\  
KelloggsToppasMini & 0.98 & 0.94 & 0.41 & 0.57 \\  
KnusperSchokoKeks & 1.0 & 1.0 & 0.92 & 0.96 \\  
KoellnMuesliKnusperHonigNuss & 1.0 & 0.97 & 1.0 & 0.99 \\  
LargeGreySpoon & 0.96 & 0.08 & 0.06 & 0.07 \\  
LinuxCup & 0.99 & 1.0 & 0.78 & 0.87 \\  
LionCerealBox & 0.99 & 0.97 & 0.85 & 0.91 \\  
MarkenSalz & 0.97 & 0.47 & 0.98 & 0.63 \\  
MeerSalz & 0.98 & 1.0 & 0.44 & 0.62 \\  
MondaminPancakeMix & 0.99 & 0.7 & 1.0 & 0.82 \\  
NesquikCereal & 0.99 & 0.96 & 0.62 & 0.75 \\  
PfannerGruneIcetea & 0.97 & 0.91 & 0.46 & 0.61 \\  
PfannerPfirsichIcetea & 0.98 & 1.0 & 0.59 & 0.74 \\  
RedMetalBowlWhiteSpeckles & 0.97 & 0.52 & 1.0 & 0.69 \\  
RedMetalCupWhiteSpeckles & 1.0 & 0.91 & 1.0 & 0.95 \\  
RedMetalPlateWhiteSpeckles & 0.97 & 1.0 & 0.07 & 0.13 \\  
RedPlasticFork & 0.98 & 0.62 & 0.78 & 0.69 \\  
RedPlasticKnife & 0.96 & 0.35 & 0.76 & 0.48 \\  
RedPlasticSpoon & 0.95 & 0.06 & 0.05 & 0.06 \\  
ReineButterMilch & 0.98 & 0.68 & 1.0 & 0.81 \\  
SeverinPancakeMaker & 0.96 & 1.0 & 0.02 & 0.04 \\  
SiggBottle & 0.97 & 1.0 & 0.04 & 0.09 \\  
SlottedSpatula & 0.97 & 0.0 & 0.0 & 0.0 \\  
SojaMilch & 1.0 & 1.0 & 0.94 & 0.97 \\  
SpitzenReis & 0.98 & 0.72 & 0.69 & 0.7 \\  
TomatoAlGustoBasilikum & 0.99 & 0.67 & 0.97 & 0.79 \\  
TomatoSauceOroDiParma & 0.99 & 1.0 & 0.82 & 0.9 \\  
VollMilch & 0.96 & 0.56 & 0.98 & 0.72 \\  
WeideMilchSmall & 0.97 & 0.67 & 0.91 & 0.77 \\  
WhiteCeramicIkeaBowl & 0.92 & 0.0 & 0.0 & 0.0 \\  
YellowCeramicPlate & 0.97 & 0.0 & 0.0 & 0.0 \\      \hline
\textbf{Gesamt}		&	\textbf{0.66}   &	\textbf{0.72}  & \textbf{0.66}     &  \textbf{0.62}     \\
\end{tabularx}
\caption[Objektinstanzen-spezifische Kenngrößen des SVMAnnotators]{Kenngrößen für die einzelnen Objekte aus der Objektinstanzen Klassifizierung der Unreal-Bilder mit dem \texttt{SVMAnnotator}.}
\label{tab:SVMClassifierGTInstance_metrics}
\end{table}


\section{10-fache Kreuzvalidierung der Unreal-Bilder mit MLNs}
\label{sec:onlyUnrealImages}
Im Folgenden werden nur Unreal-Bilder zum trainieren und testen eines \gls{mln} verwendet. Als \gls{gt} dienen die Objektklassen oder Instanznamen. Die logischen Prädikate wurden dazu mit der in Kapitel \ref{sec:analysisengine} vorgestellten \gls{ae} aus den 570 Unreal-Bildern extrahiert. Diese werden wie in Tabelle \ref{tab:annotators} auf S.\pageref{tab:annotators} beschrieben für das Modell deklariert.  Zusätzlich gibt es noch das $scene$-Prädikat und eine Einschränkung für das $object$-Prädikat:
\begin{itemize}
\item das $scene(scene)$ Prädikat ordnet die Szene in einen räumlichen Kontext ein. Die Domäne für das Prädikat ist $dom(scene) = \{breakfast, cooking, fridge\}$
\item das Prädikat $object$ wird folgendermaßen definiert: $object(cluster, object!)$. Der \glqq!\grqq \ Operator besagt, dass dieses Prädikat als funktionelle Einschränkung behandelt werden soll. Das bedeutet, dass immer exakt ein Atom wahr sein muss; alle anderen sind falsch.\footnote{\url{http://pracmln.org/mln_syntax.html}} Im Falle des obigen Prädikats bedeutet das, dass jedem Cluster genau eine Objektklasse zugeordnet sein muss. Dies macht im Rahmen des Modells Sinn, da ein Objekt nicht mehreren Klassen zugleich angehören kann und wurde auch von Nyga et al.\cite{pr2looking} in ihrer \gls{mln}-Deklaration angenommen.
\end{itemize}
Das Folgende \gls{mln} beschreibt die Zusammenhänge der einzelnen Informationen der Annotatoren und der Objektklassen:
\begin{align*}
& w_{1} \ shape(?c, +?sha) \wedge object(?c, +?obj) \\
& w_{2} \ color(?c, +?col) \wedge object(?c, +?obj) \\
& w_{3} \ size(?c, +?size) \wedge object(?c, +?obj) \\
& w_{4} \ instance(?c, +?inst) \wedge object(?c, +?obj) \\
& w_{5} \ goggles\_Logo(?c, +?comp) \wedge object(?c, +?obj)\\
& w_{6} \ goggles\_Text(?c, +?text) \wedge object(?c, +?obj)\\
& w_{7} \ goggles\_Product(?c, +?prod) \wedge object(?c, +?obj)\\
& w_{8} \ scene(+?s) \wedge object(?c, +?obj)\\
& w_{9} \ object(?c1, +?t1) \wedge object(?c2, +?t2) \wedge ?c1 =/= ?c2
\end{align*}
\textit{Hinweise zur Syntax von \pracmln:} Jeder Variable muss ein \glqq ?\grqq \ vorangestellt werden. Das \glqq +\grqq \ bedeutet, dass für jedes Element der Domäne dieser Variable eine Formel erstellt wird. \footnote{\url{http://pracmln.org/mln_syntax.html}}  \par
 
Ein \gls{mln} wurde mit \textit{Discriminative Pseudo-likelihood Learning with Custom Grounding} trainiert. Es wurde die \textit{Discrimitive} Variante des Lernalgorithmus gewählt, da es ein genaueres Modell und geringeren Rechenaufwand bietet. Als Voraussetzung müssen bestimmte Prädikate bei den Anfragen entweder nur in den Anfragen oder der Evidenzen vorkommen. Da in diesem Fall die Objektklasse erkannt werden soll, ist das $object$-Prädikat die einzige Anfrage and das trainierte \gls{mln}. Die \textit{Custom Grounding} Variante bietet eine schnellere Rechenzeit, wenn die Formeln größtenteils aus Konjunktionen bestehen. Eine Regularisierung findet durch den Gaussian-Prior mit einem Mittelwert $\mu = 0$ und einer Standardabweichung $\sigma = 10$ statt. \par   

Zum Testen werden Anfragen, um welche Objekte es sich bei den Clustern handelt, an das trainierte \gls{mln} gestellt. Es soll also die bedingte Wahrscheinlichkeit $P(object \mid E)$ berechnet werden. Es wird eine Evidenz angegeben, die von der Form her den Trainingsdaten entspricht, also eine wahrgenommene Szene darstellt, nur ohne das $object$-Prädikat, da die Objektklasse sonst schon bekannt wäre und nicht aus den anderen Eigenschaften darauf geschlossen werden muss. Als Inferenzmechanismus wird \textit{WCSP} verwendet. Dies ist ein Algorithmus, der die \textit{Most Probable Explanation (MPE)} berechnet, also nicht die gesamte bedingte Wahrscheinlichkeit, sondern nur die wahrscheinlichste Variablenbelegung unter Bedingung der Evidenz. WCSP wandelt das instanziierte \gls{mn} dazu in ein \textit{gewichtetes Constraint Satisfaction Problem} um. \par

Um dieses Modell zu evaluieren, wird 10-fache Kreuzvalidierung durchgeführt. Dabei wird der gesamte Datensatz, also alle 570 Bilder, in 10 Teilmengen aufgeteilt. Jedes Bild bleibt für den gesamten Prozess in seiner Teilmenge. Nun werden 9 Teilmengen als Trainingsdaten für ein \gls{mln} benutzt, während die übrige 10. Teilmenge zum Testen zur Verfügung steht. Dieser Vorgang wird 10 mal durchgeführt, sodass jede Teilmenge genau einmal zum Testen verwendet wurde. Die Ergebnisse der einzelnen Durchgänge können nun gemittelt werden. Dies verhindert Überanpassung des Modells, also die Anpassung an die Trainingsdaten und damit einen Verlust der Generalität des Modells. 

\subsection{Objektklassen}

Die Ergebnisse der Kreuzvalidierung mit Objektklassen als \gls{gt} sind in Abbildung \ref{fig:UnrealGTClass_confMatrix} und Tabelle \ref{tab:UnrealGTClass_metrics} dargestellt. Insgesamt werden eine hohe \gls{accuracy} für alle Objektklassen als auch Werte über 90\% für \gls{precision}, \gls{recall} und \gls{f1score} erreicht. Einzig das Besteck und der Reis schneiden etwas schlechter ab. Bei dem Besteck war dies erwartet, da Messer, Gabeln und Löffeln kleiner sind als viele andere Objekte und damit visuelle Eigenschaften schwieriger auszumachen sind. Im Gegensatz zu den Ergebnissen der \glspl{klassifikator} ist eine deutliche Verbesserung zu erkennen. Vor allem gibt es keine Objekte, die gar nicht erkannt werden. \todo{Die bessere Erkennungsrate war jedoch erwartet, da \glspl{mln} die Ergebnisse verschiedener Experten, also Annotatoren in \robosherlock, kombiniert und solche Ensembles, wie bereits zuvor dargelegt, zu besseren Ergebnissen kommen können. \newline
In den Abbildungen \ref{fig:singleEvidences} und \ref{fig:singleEvidencesGog} sind die Ergebnisse der Erkennung mit nur jeweils einer Evidenz, also einem logischen Prädikat, abgebildet. Es ist zu erkennen, dass bei Farbe, Form und Größe einige bestimmte Objekte bei der entsprechenden Evidenz präferiert werden. Bei der Instanz fällt auf, dass vieles als PancakeMaker eingeordnet wird, während viele Objekt auch richtig zugeordnet sind, was auf eine schlechte Performance des entsprechenden \gls{klassifikator}s hinweist. Der \texttt{GogglesAnnotator} annotiert vor allem texturierte Objekte, jedoch auch einige andere, welche er dann auch selten wiedererkennt. Insgesamt unterstützen diese Ergebnisse die Idee einer besseren Erkennung durch Ensembles, da jeder Annotator seine stärken und Schwächen aufweist, und erst durch die Zusammenführung der einzelnen Ergebnisse eine gute Erkennungsrate erreicht wird.}

\begin{figure}
	\includegraphics[scale=.5]{img/chapter6/UnrealGTClass.png}
\caption[Konfusionsmatrix des gesamten Unreal-Bilder Datensatzes]{Die Konfusionsmatrix für die 10-fache Kreuzvalidierung der gesamten 570 Unreal-Bilder.}
\label{fig:UnrealGTClass_confMatrix}
\end{figure}

\begin{table}
\rowcolors{1}{}{lightgray}
\begin{tabularx}{\textwidth}{Xllll}
\textbf{Objekt}	& \textbf{\gls{accuracy}} & \textbf{\gls{precision}}	& \textbf{\gls{recall}}	& \textbf{\gls{f1score}} \\ \hline
Bowl & 0.99 & 0.95 & 0.98 & 0.97 \\  
BreakfastCereal & 1.0 & 1.0 & 0.98 & 0.99 \\  
Buttermilk & 1.0 & 1.0 & 1.0 & 1.0 \\  
Coffee & 1.0 & 1.0 & 1.0 & 1.0 \\  
Cup & 1.0 & 1.0 & 1.0 & 1.0 \\  
DinnerPlate & 0.99 & 0.95 & 0.95 & 0.95 \\  
DrinkingBottle & 1.0 & 0.89 & 0.91 & 0.9 \\  
DrinkingMug & 0.99 & 0.97 & 0.9 & 0.94 \\  
Fork & 0.99 & 0.89 & 0.89 & 0.89 \\  
Juice & 1.0 & 1.0 & 0.98 & 0.99 \\  
Knife & 0.99 & 0.86 & 0.87 & 0.86 \\  
Milk & 0.99 & 0.97 & 0.98 & 0.98 \\  
PancakeMaker & 0.99 & 0.83 & 1.0 & 0.91 \\  
PancakeMix & 1.0 & 0.91 & 0.93 & 0.92 \\  
Rice & 0.99 & 0.87 & 0.91 & 0.89 \\  
Spatula & 1.0 & 0.91 & 0.91 & 0.91 \\  
Spoon & 0.99 & 0.94 & 0.89 & 0.92 \\  
TableSalt & 0.99 & 0.94 & 0.9 & 0.92 \\  
Tea-Iced & 0.99 & 0.98 & 0.95 & 0.96 \\  
TomatoSauce & 0.99 & 0.91 & 0.92 & 0.91 \\  \hline
\textbf{Gesamt}		&	\textbf{0.95}   &	\textbf{0.95}  & \textbf{0.95}     &  \textbf{0.95}     \\
\end{tabularx}
\caption[Objekt-spezifische Kenngrößen des gesamten Unreal-Bilder Datensatzes]{Kenngrößen für die einzelnen Objekte aus der 10-fachen Kreuzvalidierung der gesamten Unreal-Bilder.}
\label{tab:UnrealGTClass_metrics}
\end{table}


%\begin{figure}
%\centering
%	\begin{subfigure}[b]{0.48\textwidth}
%		\includegraphics[scale=.27]{img/chapter6/unrealEx1_color}
%		\subcaption{Farbe}
%	\end{subfigure}
%	\begin{subfigure}[b]{0.48\textwidth}
%		\includegraphics[scale=.27]{img/chapter6/unrealEx1_shape}	
%		\subcaption{Form}
%	\end{subfigure}
%	\begin{subfigure}[b]{0.48\textwidth}
%		\includegraphics[scale=.27]{img/chapter6/unrealEx1_size}	
%		\subcaption{Größe}
%	\end{subfigure}
%	\begin{subfigure}[b]{0.48\textwidth}
%		\includegraphics[scale=.27]{img/chapter6/unrealEx1_instance}	
%		\subcaption{Instanz}
%	\end{subfigure}
%\caption[Konfusionsmatrizen für die Klassifikation mit Evidenz von nur einem Experten]{\todo{Konfusionsmatrizen für die 10-fache Kreuzvalidierung mit Evidenz von nur jeweils einem Experten.}}
%\label{fig:singleEvidences}
%\end{figure}

%\begin{figure}
%	\includegraphics[scale=.27]{img/chapter6/unrealEx1_goggles}	
%\caption[Konfusionsmatrix für die Klassifikation nur durch den \texttt{GogglesAnnotator}]{\todo{Konfusionsmatrix für die 10-fache Kreuzvalidierung nur durch den \texttt{GogglesAnnotator}}}
%\label{fig:singleEvidencesGog}
%\end{figure}

\subsection{Instanznamen}

\todo{to come}

\section{Reale Bilder als Testdaten}

Im folgenden Experiment wird ein \gls{mln} mit den Unreal-Bildern trainiert und mit den realen Bildern getestet. Die Unreal-Bilder wurden wie zuvor von der in Kapitel \ref{sec:analysisengine} beschriebenen \gls{ae} annotiert. Die realen Bilder ebenfalls, allerdings ohne den \texttt{UnrealGTAnnotator} zu verwenden, da dieser nur für Bilder aus der \unreal geeignet ist, da echte Bilder keine Asset-Namen zur Bestimmung der \gls{gt} aufweisen. Dementsprechend wurde die \gls{gt} manuell annotiert. Damit die Atome für den \texttt{GogglesAnnotator} in beiden Datensätzen übereinstimmen, wurde das Clustering mit den Annotationen aus beiden Datensätzen durchgeführt. Die Deklaration des \gls{mln} und Parameter beim Lernen sind unverändert wie im vorherigen Experiment, bei dem nur Unreal-Bilder zum Einsatz kamen. Die Bedingungen und Parameter für die Anfragen sind ebenfalls gleich \par

\subsection{Objektklassen}

Bei der Klassifizierung der Objektklassen (Abbildung \ref{fig:UnrealRealGTClass_confMatrix}, Tabelle \ref{tab:UnrealRealGTClass_metrics}) ist die \gls{accuracy} für alle Klassen über 90\%, während auch die Werte für \gls{precision}, \gls{recall} und \gls{f1score} in den meisten Fällen mit über 70\% relativ hoch ausfallen. Ausnahmen bilden wie erwartet das Geschirr, aber auch die Trinkflasche, PancakeMix und Maker und der Reis. Interessanterweise schneiden Löffel und Gabeln dabei deutlich besser ab als Messer. Besonders schlecht schneidet auch der Pfannenwender ab, was auch schon bei den \glspl{klassifikator} aufgefallen ist. Da die 10-fache Kreuzvalidierung, bei der nur Unreal-Bilder verwendet wurden, dieses Problem nicht zeigt, kann angenommen werden, dass das 3D-Modell keine gute Repräsentation des echten Objektes ist. Dass die Performance bei Reis so stark abfällt, ist interessant, da die Erkennungsrate für andere texturierte Objekte (Müsli, Milch, Salz, Eistee, Tomaten Sauce) zwar auch sinkt, jedoch nicht so stark, wie beim Reis. Der wahrscheinlich wegen seiner Form von den \glspl{klassifikator} noch gut erkannte PancakeMix, scheint diesen Vorteil hier nicht ausspielen zu können, während das für die Buttermilch mit einer 100 prozentigen Erkennungsrate nicht gilt. Für den PancakeMaker scheint das Selbe zu gelten, wie für den Pfannenwender, da auch er schon bei den \glspl{klassifikator} eine Problemquelle darstellte. 

\begin{figure}
	\includegraphics[scale=.4]{img/chapter6/UnrealRealGTClass.png}
\caption[Konfusionsmatrix der Klassifikation mit Unreal-Trainingsset und Real-Testset]{Die Konfusionsmatrix für die Klassifikation aller realen Bilder durch ein \gls{mln}, das mit allen Unreal-Bildern trainiert wurde.}
\label{fig:UnrealRealGTClass_confMatrix}
\end{figure}  

\begin{table}
\rowcolors{1}{}{lightgray}
\begin{tabularx}{\textwidth}{Xllll}
\textbf{Objekt}	& \textbf{\gls{accuracy}} & \textbf{\gls{precision}}	& \textbf{\gls{recall}}	& \textbf{\gls{f1score}} \\ \hline
Bowl & 0.98 & 0.88 & 0.99 & 0.93 \\  
BreakfastCereal & 0.96 & 0.8 & 0.96 & 0.87 \\  
Buttermilk & 1.0 & 1.0 & 1.0 & 1.0 \\  
Coffee & 0.99 & 1.0 & 0.81 & 0.9 \\  
Cup & 0.99 & 0.97 & 0.92 & 0.95 \\  
DinnerPlate & 0.96 & 0.67 & 0.86 & 0.75 \\  
DrinkingBottle & 0.98 & 0.73 & 0.41 & 0.52 \\  
DrinkingMug & 1.0 & 0.95 & 0.88 & 0.91 \\  
Fork & 0.96 & 0.61 & 0.6 & 0.6 \\  
Juice & 0.99 & 1.0 & 0.74 & 0.85 \\  
Knife & 0.94 & 0.37 & 0.27 & 0.31 \\  
Milk & 0.96 & 0.99 & 0.77 & 0.86 \\  
PancakeMaker & 0.98 & 0.7 & 0.47 & 0.56 \\  
PancakeMix & 0.98 & 0.57 & 0.33 & 0.42 \\  
Rice & 0.97 & 0.35 & 0.5 & 0.41 \\  
Spatula & 0.95 & 0.16 & 0.25 & 0.2 \\  
Spoon & 0.95 & 0.75 & 0.66 & 0.7 \\  
TableSalt & 0.95 & 0.66 & 0.81 & 0.72 \\  
Tea-Iced & 0.96 & 0.75 & 0.72 & 0.73 \\  
TomatoSauce & 0.97 & 0.63 & 0.89 & 0.74 \\   \hline
\textbf{Gesamt}		&	\textbf{0.76}   &	\textbf{0.78}  & \textbf{0.76}     &  \textbf{0.76}     \\
\end{tabularx}
\caption[Objekt-spezifische Kenngrößen der Klassifikation mit Unreal-Trainingsset und Real-Testset]{Kenngrößen für die einzelnen Objekte der Klassifikation der realen Bilder durch ein \gls{mln}, das mit allen Unreal-Bildern trainiert wurde.}
\label{tab:UnrealRealGTClass_metrics}
\end{table}

\subsection{Instanznamen}

\todo{to come}